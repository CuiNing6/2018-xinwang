{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 160)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train_xy.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 159)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/train_x.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>cust_group</th>\n",
       "      <th>y</th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>...</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110000</td>\n",
       "      <td>group_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.604988</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110001</td>\n",
       "      <td>group_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110002</td>\n",
       "      <td>group_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.565979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110003</td>\n",
       "      <td>group_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.316209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110004</td>\n",
       "      <td>group_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cust_id cust_group  y       x_1       x_2  x_3  x_4  x_5  x_6  x_7  ...    \\\n",
       "0   110000    group_3  0  0.354167  0.604988  -99  -99  -99  -99  -99  ...     \n",
       "1   110001    group_3  0  0.125000  0.012058  -99  -99  -99  -99  -99  ...     \n",
       "2   110002    group_3  0  0.333333  0.565979    0    0    0    0    0  ...     \n",
       "3   110003    group_3  0  0.208333  0.316209    0    0    0    0    1  ...     \n",
       "4   110004    group_3  0  0.208333  0.008061  -99  -99  -99  -99  -99  ...     \n",
       "\n",
       "   x_148  x_149  x_150  x_151  x_152  x_153  x_154  x_155  x_156  x_157  \n",
       "0      1      1      1      1      1      1      1      1      3    -99  \n",
       "1      1      1      1      1      1      1      1      1      2      2  \n",
       "2      1      1      2      1      1      1      1      1      2      2  \n",
       "3      2      1      1      1      1      1      1      1      2      4  \n",
       "4      1      1      1      1      1      1      1      1      2      1  \n",
       "\n",
       "[5 rows x 160 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b1dcca6b00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAFKCAYAAAAe6CY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1da09dcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['y'].value_counts().plot.pie(autopct = '%1.2f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train.drop(['cust_group','y','cust_id'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = test.drop(['cust_group','cust_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 157)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 157)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 157)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.concat([x_train,x_test])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(96,158):\n",
    "    col = 'x'+'_'+str(i)\n",
    "    dummies_df = pd.get_dummies(x[col]).rename(columns=lambda x: col + str(x))\n",
    "    x = pd.concat([x, dummies_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_1561</th>\n",
       "      <th>x_1562</th>\n",
       "      <th>x_1563</th>\n",
       "      <th>x_157-99</th>\n",
       "      <th>x_1571</th>\n",
       "      <th>x_1572</th>\n",
       "      <th>x_1573</th>\n",
       "      <th>x_1574</th>\n",
       "      <th>x_15710</th>\n",
       "      <th>x_15711</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.604988</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.565979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.316209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 361 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2  x_3  x_4  x_5  x_6  x_7  x_8  x_9  x_10   ...     \\\n",
       "0  0.354167  0.604988  -99  -99  -99  -99  -99  -99  -99   -99   ...      \n",
       "1  0.125000  0.012058  -99  -99  -99  -99  -99  -99  -99   -99   ...      \n",
       "2  0.333333  0.565979    0    0    0    0    0    0    0     0   ...      \n",
       "3  0.208333  0.316209    0    0    0    0    1    1    0     0   ...      \n",
       "4  0.208333  0.008061  -99  -99  -99  -99  -99  -99    0     1   ...      \n",
       "\n",
       "   x_1561  x_1562  x_1563  x_157-99  x_1571  x_1572  x_1573  x_1574  x_15710  \\\n",
       "0       0       0       1         1       0       0       0       0        0   \n",
       "1       0       1       0         0       0       1       0       0        0   \n",
       "2       0       1       0         0       0       1       0       0        0   \n",
       "3       0       1       0         0       0       0       0       1        0   \n",
       "4       0       1       0         0       1       0       0       0        0   \n",
       "\n",
       "   x_15711  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 361 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 361)\n",
      "(10000, 361)\n"
     ]
    }
   ],
   "source": [
    "train_X = x[0:15000]\n",
    "test_X = x[15000:25000]\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor = train_X\n",
    "# cor['y'] = Y_train\n",
    "# cor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corrr = cor.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns = corrr[corrr['y']>0.0].index.values\n",
    "# len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_feature = train_X[columns]\n",
    "# print(train_feature.shape)\n",
    "\n",
    "# train_feature = train_feature.drop(['y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column = columns[0:132]\n",
    "# test_feature = test_X[column]\n",
    "# print(test_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics  #accuracy_score,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import label_propagation\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Reshape, Flatten, MaxPool2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val= train_test_split(train_X,Y_train,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data_):\n",
    "    idx_1 = data_[data_['label']==0].index\n",
    "    idx_2 = data_[data_['label']==1].index\n",
    "    nb_1 = len(data_.loc[idx_1])\n",
    "    nb_2 = len(data_.loc[idx_2])\n",
    "#     print(nb_1)\n",
    "#     print(nb_2)\n",
    "    idx_list_1 = list(idx_1)\n",
    "    idx_list_2 = list(idx_2)\n",
    "    train_x1 = data_.loc[idx_list_1]\n",
    "    train_x2 = data_.loc[idx_list_2]\n",
    "#     print(train_x1.shape)\n",
    "#     print(train_x2.shape)\n",
    "    return train_x1,train_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample_data(data, number):\n",
    "    idx_1 = data.index\n",
    "    nb_1 = len(idx_1)\n",
    "#     print(nb_1)\n",
    "#     number = int(nb_1 * rate)\n",
    "    idx_1_sub = np.random.choice(idx_1, number)\n",
    "    print(idx_1_sub)\n",
    "    nb_2 = len(data.loc[idx_1_sub])\n",
    "#     print(nb_2)\n",
    "    idx_list_1 = list(idx_1_sub)\n",
    "    train_1 = data.loc[idx_1_sub]\n",
    "#     print(train_1.shape)\n",
    "    return train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_data(train_x1, train_x2):\n",
    "    train_data1 = train_x1.drop(['label'],axis =1)\n",
    "    train_y1 = train_x1['label']\n",
    "    \n",
    "    train_data2 = train_x2.drop(['label'],axis =1)\n",
    "    train_y2 = train_x2['label']\n",
    "    \n",
    "    train_data = train_data1.append(train_data2)\n",
    "    train_y = train_y1.append(train_y2)\n",
    "    \n",
    "    return train_data, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_1562</th>\n",
       "      <th>x_1563</th>\n",
       "      <th>x_157-99</th>\n",
       "      <th>x_1571</th>\n",
       "      <th>x_1572</th>\n",
       "      <th>x_1573</th>\n",
       "      <th>x_1574</th>\n",
       "      <th>x_15710</th>\n",
       "      <th>x_15711</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.389913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9804</th>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.425099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.491195</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.361088</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x_1       x_2  x_3  x_4  x_5  x_6  x_7  x_8  x_9  x_10  ...    \\\n",
       "1154  0.291667  0.389913    0    0    0    0    3    2    0     0  ...     \n",
       "9804  0.208333  0.425099    0    0    0    0    4    2    0     0  ...     \n",
       "7919  0.083333  0.491195  -99  -99  -99  -99  -99  -99  -99   -99  ...     \n",
       "4854  0.291667  0.361088  -99  -99  -99  -99  -99  -99  -99   -99  ...     \n",
       "5947  0.250000  0.272055    0    0    0    0    2    2    0     0  ...     \n",
       "\n",
       "      x_1562  x_1563  x_157-99  x_1571  x_1572  x_1573  x_1574  x_15710  \\\n",
       "1154       0       0         0       0       0       0       1        0   \n",
       "9804       1       0         1       0       0       0       0        0   \n",
       "7919       1       0         1       0       0       0       0        0   \n",
       "4854       0       0         1       0       0       0       0        0   \n",
       "5947       1       0         1       0       0       0       0        0   \n",
       "\n",
       "      x_15711  label  \n",
       "1154        0      0  \n",
       "9804        0      1  \n",
       "7919        0      0  \n",
       "4854        0      0  \n",
       "5947        0      0  \n",
       "\n",
       "[5 rows x 362 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = X_train\n",
    "xx['label'] = y_train\n",
    "\n",
    "xx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11442, 362)\n",
      "(558, 362)\n"
     ]
    }
   ],
   "source": [
    "train_x1, train_x2 = split_data(xx)\n",
    "print(train_x1.shape)\n",
    "print(train_x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10826  5986  2817  1347  7479  1240  7031 12268   679  5903 10376 13511\n",
      "  7791 11481  3361 12198  6205  5101 10781  7549 12916 11814  2350   406\n",
      " 11837  1366  4768 12553  3748   491 11712 13288 13443  8160   618 14156\n",
      "  9039 11008 11678 11172  7821 10418 10046  6189  2061  7774  4524 11652\n",
      " 13412 12918   914 10256 12349 10239 12844  4365  8230 13820 14279 12844\n",
      " 10610  5612  8959 10734 12198  8331  7974 14697  5021  7854   228 11752\n",
      "  5529   427  2563 12678  5429 13977  1941  2146  2397   922 10306  6638\n",
      " 12583  3421  8187  3825 11701 11893  7446  4527  1717  1691 12845  7711\n",
      " 12433 13352 14126  8777 12088  5858   534  4596  6227 12671  8299 13263\n",
      " 12105 11950 14855  8785   356  4344  9556  3391 13666  7401 14058  9447\n",
      " 10791 10113 10975 14817  1775 12259 14086 13530 11217 12766 14762 13638\n",
      "  9144 13465 11282  7709 12492 14753  3887   732  1959  4344 14478 13098\n",
      "  3225  7647  1911  3201  6885 11752  5843  4036 14374   944  2120   263\n",
      " 11705 13008 12120  2782  3063  6126   745  2007  5617  2915  5014  1568\n",
      "  3343  4469   722  8846  1126 12104 12844  9089 13383 11026  3429   279\n",
      "  4160 14561  8415  3763   574 10634  2108  5056  6233  2434 13880 11111\n",
      "  3766  2113   472  3709 11768  4591  7485  9056   311   123 11231 10056\n",
      " 14143  4353  2921 13651  5291 14476  8586  8436  4848  5091   919  4765\n",
      "  8054  3152  4074  5816 11127  3814  3496  3223 11237 12250  1759 11515\n",
      " 12197  8035 12924  8931   188 13776  9460  9536  8043 14348  8673  8888\n",
      "  1581 14561 14448  4498  3861  8187  4128 11137  6451  7397  5289   427\n",
      " 11228 13352  7351  8924   482 10943   139 10764  1386  4179  1140 11425\n",
      "  9053 11148  6377 13169   759  6038 11026  3364 13268   581  2463  1459\n",
      " 10970  7045 12712  1700  5181 10731 13469  5899  2754  4665  4132 13757\n",
      "  9358 12931  3547  7793 14721  5978  7339  6425  6151  2266  1323 10895\n",
      " 12605  4089   443  8300 13277  5181  6553  1017  1770  9110 10795  6391\n",
      "  1773 10592  4056  3808  1033 10011 14643  4130  6111 11654  3304  4037\n",
      " 12658 11943  8590  3323 11335 10251 12815  4180 11745 13515 10598 13872\n",
      " 10921 13682   537  4525  1489  6012 10272 13651 13596  7342 13112 12027\n",
      "   608  5036 12136 10325 14201   686  8957 14539 10611 14561  7680 13497\n",
      "  2124 10291 14213 12959  4410 11520 10067  1314  4057 13841  8468  1059\n",
      "  4881  4126 14729  1521 13466  4220  8306 11128  7672 11521  1597  8373\n",
      "  5589  8913 13338  7071 14681  8485  6026  7960  1274 11120  4416 10637\n",
      " 13635  4600  5651  4466  2830 10460  4272  1688  7726   554 11239  6464\n",
      " 12538  5518 11221 12200 11704   455  1208  8080 10125  8496 12827 13453\n",
      "  3761  2576  3934  3138  8774 13555  4739   734  2544 12838  3783  6099\n",
      " 11774  7643 12595  2727  6485 13799  5504  8675  3492   695  2295  1350\n",
      "  5694  5494 12355  2317  5734  6782   628 11318  1865 14410   788 11872\n",
      " 10696 14091 12612  1407  5888   519  9268 11384  9314 12134  2031  2386\n",
      "  7916 12300 13126 11857  4193 11321  2310   928   571  3421 12503  4271\n",
      " 13691 12270 14689  2558  9263  8912  3088  9004  1805 10057  2647 12618\n",
      "  7121  4154   600  6885  9316 13691 10100 11069  3234  2403  1561 10888\n",
      "  9098 10305  2967  8721  8796 10111  7407 11319  1635  5789  6481  9228\n",
      " 13276  6166 10605  9071 14781  9048  3794  4474  7625 13651 13261  5409\n",
      "  5223 11666 10180  8555  4027  5614  5117 14242 13791   768 11704  3291\n",
      "  5084 13833   606  2358  8436  4901 12673  1171  3268  7117 12834 14498\n",
      "  3623  6428  4901 10359  1332 10837  8441 11735]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train1 = resample_data(train_x1, 560)\n",
    "print(train1.shape)\n",
    "multi_x1, multi_y1 = concat_data(train1, train_x2)\n",
    "print(multi_x1.shape)\n",
    "print(multi_y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.81      2867\n",
      "    class 1       0.10      0.73      0.17       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.6863333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm1_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x1, multi_y1)\n",
    "predictions1_1 = gbm1_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions1_1, target_names=target_names))\n",
    "\n",
    "val1_1_acc = metrics.accuracy_score(y_val,predictions1_1)#验证集上的auc值\n",
    "print(val1_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.81      2867\n",
      "    class 1       0.10      0.71      0.17       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.6973333333333334\n"
     ]
    }
   ],
   "source": [
    "clf1_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x1, multi_y1)\n",
    "predictions1_2 = clf1_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions1_2, target_names=target_names))\n",
    "\n",
    "val1_2_acc = metrics.accuracy_score(y_val,predictions1_2)#验证集上的auc值\n",
    "print(val1_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 3s 3ms/step - loss: 0.6728 - acc: 0.5577 - val_loss: 0.3159 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.6253 - acc: 0.6412 - val_loss: 0.0747 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.6001 - acc: 0.6620 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5878 - acc: 0.6740 - val_loss: 0.0584 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5753 - acc: 0.6829 - val_loss: 0.1454 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5659 - acc: 0.6948 - val_loss: 0.2105 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5693 - acc: 0.7097 - val_loss: 0.2758 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5413 - acc: 0.7247 - val_loss: 0.3934 - val_acc: 0.9464\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5345 - acc: 0.7455 - val_loss: 0.0810 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5549 - acc: 0.7147 - val_loss: 0.3024 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5411 - acc: 0.7217 - val_loss: 0.5124 - val_acc: 0.8125\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5259 - acc: 0.7256 - val_loss: 0.2534 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5226 - acc: 0.7346 - val_loss: 0.1931 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5291 - acc: 0.7256 - val_loss: 0.4115 - val_acc: 0.8750\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5183 - acc: 0.7425 - val_loss: 0.2906 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5227 - acc: 0.7386 - val_loss: 0.4406 - val_acc: 0.8304\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5210 - acc: 0.7396 - val_loss: 0.2932 - val_acc: 0.9911\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5109 - acc: 0.7465 - val_loss: 0.7507 - val_acc: 0.4732\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5323 - acc: 0.7346 - val_loss: 0.5690 - val_acc: 0.7500\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5081 - acc: 0.7505 - val_loss: 0.5075 - val_acc: 0.7589\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5089 - acc: 0.7525 - val_loss: 0.4502 - val_acc: 0.7946\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5153 - acc: 0.7475 - val_loss: 0.5339 - val_acc: 0.7232\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5154 - acc: 0.7455 - val_loss: 0.7237 - val_acc: 0.5446\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5102 - acc: 0.7425 - val_loss: 0.6134 - val_acc: 0.6339\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4996 - acc: 0.7575 - val_loss: 0.7366 - val_acc: 0.5357\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5061 - acc: 0.7505 - val_loss: 0.6730 - val_acc: 0.5714\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4977 - acc: 0.7475 - val_loss: 0.7003 - val_acc: 0.5625\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5009 - acc: 0.7495 - val_loss: 0.7237 - val_acc: 0.5625\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4909 - acc: 0.7565 - val_loss: 0.5287 - val_acc: 0.6964\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5085 - acc: 0.7376 - val_loss: 0.5985 - val_acc: 0.6339\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5020 - acc: 0.7425 - val_loss: 0.6547 - val_acc: 0.5982\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4951 - acc: 0.7614 - val_loss: 0.6176 - val_acc: 0.6250\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4834 - acc: 0.7445 - val_loss: 0.6264 - val_acc: 0.6071\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4911 - acc: 0.7555 - val_loss: 0.5623 - val_acc: 0.6607\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4838 - acc: 0.7664 - val_loss: 0.5204 - val_acc: 0.6875\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4702 - acc: 0.7684 - val_loss: 0.5967 - val_acc: 0.6250\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4818 - acc: 0.7724 - val_loss: 0.5780 - val_acc: 0.6250\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4941 - acc: 0.7545 - val_loss: 0.6439 - val_acc: 0.5893\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5036 - acc: 0.7495 - val_loss: 0.6166 - val_acc: 0.5982\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4881 - acc: 0.7714 - val_loss: 0.6874 - val_acc: 0.5268\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4999 - acc: 0.7724 - val_loss: 0.5862 - val_acc: 0.6250\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4724 - acc: 0.7753 - val_loss: 0.6619 - val_acc: 0.5804\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4906 - acc: 0.7724 - val_loss: 0.6958 - val_acc: 0.5179\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4627 - acc: 0.7734 - val_loss: 0.6923 - val_acc: 0.5268\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4709 - acc: 0.7545 - val_loss: 0.6729 - val_acc: 0.5625\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4768 - acc: 0.7674 - val_loss: 0.6919 - val_acc: 0.5179\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4655 - acc: 0.7773 - val_loss: 0.6223 - val_acc: 0.5982\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4695 - acc: 0.7624 - val_loss: 0.6812 - val_acc: 0.5357\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4832 - acc: 0.7545 - val_loss: 0.6409 - val_acc: 0.5893\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4664 - acc: 0.7773 - val_loss: 0.6670 - val_acc: 0.5536\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4663 - acc: 0.7664 - val_loss: 0.6324 - val_acc: 0.5625\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4784 - acc: 0.7664 - val_loss: 0.6958 - val_acc: 0.5536\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4627 - acc: 0.7773 - val_loss: 0.7057 - val_acc: 0.5536\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4640 - acc: 0.7803 - val_loss: 0.6963 - val_acc: 0.5446\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4630 - acc: 0.7694 - val_loss: 0.6297 - val_acc: 0.6161\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4482 - acc: 0.7833 - val_loss: 0.6936 - val_acc: 0.5357\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4604 - acc: 0.7753 - val_loss: 0.6348 - val_acc: 0.6071\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4471 - acc: 0.7893 - val_loss: 0.6881 - val_acc: 0.5446\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4606 - acc: 0.7883 - val_loss: 0.6862 - val_acc: 0.5536\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4636 - acc: 0.7843 - val_loss: 0.6035 - val_acc: 0.6429\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4545 - acc: 0.7942 - val_loss: 0.7160 - val_acc: 0.5625\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4597 - acc: 0.7714 - val_loss: 0.6348 - val_acc: 0.6250\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4427 - acc: 0.7903 - val_loss: 0.7247 - val_acc: 0.5268\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4679 - acc: 0.7753 - val_loss: 0.6426 - val_acc: 0.5982\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4467 - acc: 0.7813 - val_loss: 0.6548 - val_acc: 0.6071\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4295 - acc: 0.7952 - val_loss: 0.6591 - val_acc: 0.6161\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4327 - acc: 0.7952 - val_loss: 0.6626 - val_acc: 0.5893\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4474 - acc: 0.7992 - val_loss: 0.7080 - val_acc: 0.5714\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4250 - acc: 0.7922 - val_loss: 0.6406 - val_acc: 0.6071\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4236 - acc: 0.7903 - val_loss: 0.7354 - val_acc: 0.5625\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4386 - acc: 0.7873 - val_loss: 0.6703 - val_acc: 0.6071\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4315 - acc: 0.8171 - val_loss: 0.7114 - val_acc: 0.5804\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4429 - acc: 0.7952 - val_loss: 0.6410 - val_acc: 0.6161\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4348 - acc: 0.7863 - val_loss: 0.7826 - val_acc: 0.5179\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4275 - acc: 0.8032 - val_loss: 0.6727 - val_acc: 0.5893\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4635 - acc: 0.7932 - val_loss: 0.7207 - val_acc: 0.5446\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4335 - acc: 0.7803 - val_loss: 0.6461 - val_acc: 0.5982\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4242 - acc: 0.7932 - val_loss: 0.7301 - val_acc: 0.5804\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4332 - acc: 0.8072 - val_loss: 0.6776 - val_acc: 0.5893\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4374 - acc: 0.7903 - val_loss: 0.6693 - val_acc: 0.6071\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4253 - acc: 0.7942 - val_loss: 0.6652 - val_acc: 0.6250\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4169 - acc: 0.7952 - val_loss: 0.7502 - val_acc: 0.5625\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3943 - acc: 0.8072 - val_loss: 0.7127 - val_acc: 0.5982\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4259 - acc: 0.7952 - val_loss: 0.7030 - val_acc: 0.5982\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4269 - acc: 0.8022 - val_loss: 0.7181 - val_acc: 0.5804\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4320 - acc: 0.7952 - val_loss: 0.7679 - val_acc: 0.5714\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4254 - acc: 0.7992 - val_loss: 0.6420 - val_acc: 0.6518\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4130 - acc: 0.8022 - val_loss: 0.7042 - val_acc: 0.6161\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4312 - acc: 0.7903 - val_loss: 0.7817 - val_acc: 0.5804\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4073 - acc: 0.8091 - val_loss: 0.6935 - val_acc: 0.6250\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4282 - acc: 0.7942 - val_loss: 0.7849 - val_acc: 0.5625\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4022 - acc: 0.8121 - val_loss: 0.6771 - val_acc: 0.6518\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4003 - acc: 0.8270 - val_loss: 0.7236 - val_acc: 0.6071\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4130 - acc: 0.7972 - val_loss: 0.7305 - val_acc: 0.6161\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3917 - acc: 0.7992 - val_loss: 0.7268 - val_acc: 0.5893\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3966 - acc: 0.8131 - val_loss: 0.7180 - val_acc: 0.6071\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3781 - acc: 0.8221 - val_loss: 0.7390 - val_acc: 0.6250\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3909 - acc: 0.8191 - val_loss: 0.7438 - val_acc: 0.6071\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3906 - acc: 0.8181 - val_loss: 0.8244 - val_acc: 0.5714\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3793 - acc: 0.8250 - val_loss: 0.6234 - val_acc: 0.6786\n"
     ]
    }
   ],
   "source": [
    "nn_x = multi_x1.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y =  multi_y1.values\n",
    "nn_y_train = to_categorical(nn_y)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x,nn_y_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.81      2867\n",
      "    class 1       0.09      0.65      0.16       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.694\n"
     ]
    }
   ],
   "source": [
    "predictions1_3 = model.predict(nn_val)\n",
    "pred1_3 = []\n",
    "for i in range(3000):\n",
    "    pred1_3.append(np.argmax(predictions1_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred1_3, target_names=target_names))\n",
    "\n",
    "val1_3_acc = metrics.accuracy_score(y_val,pred1_3)#验证集上的auc值\n",
    "print(val1_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.82      2867\n",
      "    class 1       0.10      0.71      0.17       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.701\n"
     ]
    }
   ],
   "source": [
    "predictions1 = predictions1_1+predictions1_2+pred1_3\n",
    "for i in range(3000):\n",
    "    if predictions1[i] >= 2:\n",
    "        predictions1[i] = 1\n",
    "    else:\n",
    "        predictions1[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions1, target_names=target_names))\n",
    "\n",
    "val1_acc = metrics.accuracy_score(y_val,predictions1)#验证集上的auc值\n",
    "print(val1_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8999  2434  7146  3729 12311  6550 12212  2558 10021 12602 14178 13558\n",
      " 12791  7349 12270  1014  8040  8018  3267   958  3972  1080  7050  3247\n",
      " 11003  9404  8487  3578 12818 12634  3480 13711 14355   917  5079  9294\n",
      "  1556  6897  4422 12102 14692  2103  2087   654  5618  4794  6313  2246\n",
      "   206  9076  7504  5151  1144  7940 10672 13654  9440  5388  2719  3635\n",
      "  6386 11840 12437  2442 14855  4791  7225 11763  4840  9366  2084  8341\n",
      " 13298  4014  3654  8682  7771  5423  4847  3082    95   296 11989  7143\n",
      "  5216 14283  1752 14538  4275  3409  6455  7683  7208 13509  8968  9129\n",
      "  5554  4884 11477 10322 12356  1828  7096 13500   669 14243  6623   507\n",
      "  1631  1068 14026  8162  2387  8266  9246  7479  1723  1768   687   374\n",
      " 11445    35 13158 10080 11849 13372 13056  7091  8166  2311 12255 11815\n",
      "  3308 11834 11896 11398  3953 12058 10576  2642 14220  3747  3568 14106\n",
      " 11345  2498  2173 12673 12406 10379  4133  9452  3748  4890  8502  4489\n",
      "  4425  6857  8367  3015  7258  1687  2706  2616 14633  9325  3956 10906\n",
      "  7447  3510 10516  3647 13835   481  2129 13315 13131  4209  4331 10737\n",
      "  6018  7024 13923  2262  1068 13383  1899 11138   910  8054 12383 13152\n",
      "  3071 13722  1242  6220  3529 14359  2780  6612 10260  7172  6308  2487\n",
      "   564 13450  8981  2740  8646 13530 13888 14426 11611  1303  2134  9208\n",
      "  6417  2324 10201  1116 13371 11297 10544  3204  5203 14157  6779  5618\n",
      " 10161  2379 12377 13715  9223 11441  4879  4472 12554  9391 12600  7712\n",
      "  3370  5182  2551 14658  6263 11100 13241   550  5963  1443  6243 13385\n",
      "  9225 10638 11719  1846 10468  9396  2557 10254  2242 10159  3136  8155\n",
      "  6724   179  2516 14832 14251  3812 12569   139   857  9465 12679  4421\n",
      "  8866   888  3723   898 11643  1811 12901 13134 12383 12741 11655 10737\n",
      " 14321  5580  1096  5307  2438 10450 11813   109 10200  4651  6192 10424\n",
      "   238   569   907 11142  8046 11165 13522  6254  6231 10956  9174  2029\n",
      " 11492 12917   165    10 13562 10957  7112  5960  8311  1885 14476  7937\n",
      "  1920  5568  8581  8519  7434 10028  4184 14567  4831  1680 10121  8957\n",
      "  3422 10888 14461  7084  6103 12284   493  5589   419  7336 12345 12835\n",
      "  2921 10082  5357 11609  8130  7298  3234  5282  1303 10991  3081  3712\n",
      " 14811 10562  1103  6668 14792  1715  4823 12286  1407 13593 10522  8257\n",
      "  8444 10286  3835 12058  1914   712   308  2362 14398   217 12830  3147\n",
      "  6308  7704  5995 11526  6969  1135  5724  1065  2912  5954 14046  3835\n",
      "  4772 10840  8331  3376  2509 12780  3467   830  7157 12586  5234  1592\n",
      "   663  7090 10380 12265  5664  2130  2297  5591 13855  6729   826  4325\n",
      "    58  8043  3148  1506  6945  3622 10923  6514 10432  7331  4341 13617\n",
      " 12532 12105 13000 11178  5717 10489  8577 10565  3724 10566  9216 12847\n",
      " 10885   936 12357  5570 11244 12764  7615  1695 10463 13523  4748  4803\n",
      "  6516  4880 11884 12623    19 14292  7151  2707  2935 13232 10172 14491\n",
      "  6405   160 13948  8831  4767  6337 14170  1715  6826  1530  2469 11659\n",
      "  4098 14684 13695  3592  9550  9070  2246  1795 13032 13479 12722  9246\n",
      "  2182  5298  2950 14539  4184 11507 14178  5326  6767  2745 12856 14009\n",
      "  7733  9023 10576 12044  4504 13981 13845 14282 10927  7342 14463  6126\n",
      "  2022  5825  2614  7372  7590 13191  2359 13274  5355  3805  7817  5954\n",
      "  8914 13727 11125  5955  2902  7481 11515  6553  4444 12469  1833  1764\n",
      "  1067 10113  3242 13775 13736  4633  7294 11295  5389 13747   963 13270\n",
      " 13065 14673  9536 14518  2584  4067]\n",
      "(558, 362)\n",
      "(1116, 361)\n",
      "(1116,)\n"
     ]
    }
   ],
   "source": [
    "train2 = resample_data(train_x1, 558)\n",
    "print(train2.shape)\n",
    "multi_x2, multi_y2 = concat_data(train2, train_x2)\n",
    "print(multi_x2.shape)\n",
    "print(multi_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.68      0.80      2867\n",
      "    class 1       0.10      0.78      0.18       133\n",
      "\n",
      "avg / total       0.95      0.68      0.78      3000\n",
      "\n",
      "0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm2_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x2, multi_y2)\n",
    "predictions2_1 = gbm2_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions2_1, target_names=target_names))\n",
    "\n",
    "val2_1_acc = metrics.accuracy_score(y_val,predictions2_1)#验证集上的auc值\n",
    "print(val2_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.67      0.80      2867\n",
      "    class 1       0.09      0.74      0.17       133\n",
      "\n",
      "avg / total       0.94      0.67      0.77      3000\n",
      "\n",
      "0.6716666666666666\n"
     ]
    }
   ],
   "source": [
    "clf2_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x2, multi_y2)\n",
    "predictions2_2 = clf2_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions2_2, target_names=target_names))\n",
    "\n",
    "val2_2_acc = metrics.accuracy_score(y_val,predictions2_2)#验证集上的auc值\n",
    "print(val2_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1004 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1004/1004 [==============================] - 4s 4ms/step - loss: 0.6654 - acc: 0.5777 - val_loss: 0.1354 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.6036 - acc: 0.6633 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5850 - acc: 0.6833 - val_loss: 0.0326 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5603 - acc: 0.7032 - val_loss: 0.0782 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5584 - acc: 0.7032 - val_loss: 0.0561 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5478 - acc: 0.7201 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5548 - acc: 0.7122 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5503 - acc: 0.7271 - val_loss: 0.0636 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5345 - acc: 0.7390 - val_loss: 0.0409 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5321 - acc: 0.7321 - val_loss: 0.0509 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5218 - acc: 0.7331 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5252 - acc: 0.7500 - val_loss: 0.0490 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5171 - acc: 0.7520 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5305 - acc: 0.7430 - val_loss: 0.0475 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5113 - acc: 0.7341 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5221 - acc: 0.7430 - val_loss: 0.0589 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5214 - acc: 0.7331 - val_loss: 0.0845 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5049 - acc: 0.7341 - val_loss: 0.1353 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5123 - acc: 0.7460 - val_loss: 0.1955 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5002 - acc: 0.7371 - val_loss: 0.0818 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5003 - acc: 0.7450 - val_loss: 0.1863 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4900 - acc: 0.7400 - val_loss: 0.1208 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5016 - acc: 0.7500 - val_loss: 0.2288 - val_acc: 0.9911\n",
      "Epoch 24/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.5098 - acc: 0.7490 - val_loss: 0.2039 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4971 - acc: 0.7420 - val_loss: 0.2396 - val_acc: 0.9732\n",
      "Epoch 26/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4883 - acc: 0.7590 - val_loss: 0.1689 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4901 - acc: 0.7550 - val_loss: 0.2286 - val_acc: 0.9911\n",
      "Epoch 28/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4922 - acc: 0.7450 - val_loss: 0.2744 - val_acc: 0.9375\n",
      "Epoch 29/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4785 - acc: 0.7620 - val_loss: 0.2966 - val_acc: 0.9375\n",
      "Epoch 30/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4855 - acc: 0.7510 - val_loss: 0.3427 - val_acc: 0.8750\n",
      "Epoch 31/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4944 - acc: 0.7550 - val_loss: 0.3822 - val_acc: 0.8750\n",
      "Epoch 32/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4869 - acc: 0.7590 - val_loss: 0.3997 - val_acc: 0.8304\n",
      "Epoch 33/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4798 - acc: 0.7679 - val_loss: 0.3814 - val_acc: 0.8214\n",
      "Epoch 34/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4759 - acc: 0.7669 - val_loss: 0.3834 - val_acc: 0.8214\n",
      "Epoch 35/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4707 - acc: 0.7679 - val_loss: 0.4274 - val_acc: 0.7679\n",
      "Epoch 36/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4667 - acc: 0.7669 - val_loss: 0.3926 - val_acc: 0.7946\n",
      "Epoch 37/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4777 - acc: 0.7620 - val_loss: 0.4273 - val_acc: 0.7857\n",
      "Epoch 38/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4769 - acc: 0.7649 - val_loss: 0.3816 - val_acc: 0.8482\n",
      "Epoch 39/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4954 - acc: 0.7580 - val_loss: 0.4171 - val_acc: 0.8571\n",
      "Epoch 40/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4574 - acc: 0.7709 - val_loss: 0.5197 - val_acc: 0.6875\n",
      "Epoch 41/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4874 - acc: 0.7550 - val_loss: 0.4888 - val_acc: 0.7232\n",
      "Epoch 42/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4694 - acc: 0.7829 - val_loss: 0.5897 - val_acc: 0.6518\n",
      "Epoch 43/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4682 - acc: 0.7580 - val_loss: 0.5776 - val_acc: 0.6607\n",
      "Epoch 44/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4508 - acc: 0.7898 - val_loss: 0.5994 - val_acc: 0.6518\n",
      "Epoch 45/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4758 - acc: 0.7729 - val_loss: 0.5741 - val_acc: 0.6696\n",
      "Epoch 46/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4731 - acc: 0.7590 - val_loss: 0.6352 - val_acc: 0.6339\n",
      "Epoch 47/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4642 - acc: 0.7769 - val_loss: 0.6343 - val_acc: 0.6339\n",
      "Epoch 48/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4544 - acc: 0.7789 - val_loss: 0.6622 - val_acc: 0.6250\n",
      "Epoch 49/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4608 - acc: 0.7709 - val_loss: 0.6189 - val_acc: 0.6518\n",
      "Epoch 50/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4590 - acc: 0.7769 - val_loss: 0.6558 - val_acc: 0.6339\n",
      "Epoch 51/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4453 - acc: 0.7839 - val_loss: 0.7182 - val_acc: 0.6161\n",
      "Epoch 52/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4462 - acc: 0.7779 - val_loss: 0.6704 - val_acc: 0.6429\n",
      "Epoch 53/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4351 - acc: 0.7759 - val_loss: 0.6917 - val_acc: 0.6161\n",
      "Epoch 54/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4411 - acc: 0.7759 - val_loss: 0.6333 - val_acc: 0.6696\n",
      "Epoch 55/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4386 - acc: 0.7958 - val_loss: 0.7183 - val_acc: 0.5982\n",
      "Epoch 56/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4473 - acc: 0.7888 - val_loss: 0.6886 - val_acc: 0.6071\n",
      "Epoch 57/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4421 - acc: 0.7869 - val_loss: 0.6196 - val_acc: 0.6429\n",
      "Epoch 58/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4377 - acc: 0.7898 - val_loss: 0.6832 - val_acc: 0.6161\n",
      "Epoch 59/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4476 - acc: 0.7779 - val_loss: 0.6682 - val_acc: 0.6518\n",
      "Epoch 60/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4238 - acc: 0.7988 - val_loss: 0.7882 - val_acc: 0.5804\n",
      "Epoch 61/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4512 - acc: 0.7719 - val_loss: 0.6531 - val_acc: 0.6429\n",
      "Epoch 62/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4441 - acc: 0.7769 - val_loss: 0.6745 - val_acc: 0.6250\n",
      "Epoch 63/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4339 - acc: 0.7968 - val_loss: 0.6610 - val_acc: 0.6607\n",
      "Epoch 64/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4353 - acc: 0.7908 - val_loss: 0.7322 - val_acc: 0.6161\n",
      "Epoch 65/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4269 - acc: 0.7988 - val_loss: 0.7236 - val_acc: 0.6161\n",
      "Epoch 66/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4447 - acc: 0.7829 - val_loss: 0.7130 - val_acc: 0.6250\n",
      "Epoch 67/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4357 - acc: 0.7878 - val_loss: 0.6425 - val_acc: 0.6429\n",
      "Epoch 68/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4215 - acc: 0.7898 - val_loss: 0.7613 - val_acc: 0.6161\n",
      "Epoch 69/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4263 - acc: 0.7958 - val_loss: 0.7048 - val_acc: 0.6250\n",
      "Epoch 70/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4180 - acc: 0.7928 - val_loss: 0.7101 - val_acc: 0.6339\n",
      "Epoch 71/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4109 - acc: 0.8068 - val_loss: 0.6812 - val_acc: 0.6518\n",
      "Epoch 72/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4149 - acc: 0.8078 - val_loss: 0.6974 - val_acc: 0.6429\n",
      "Epoch 73/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3890 - acc: 0.8237 - val_loss: 0.7015 - val_acc: 0.6429\n",
      "Epoch 74/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4241 - acc: 0.7968 - val_loss: 0.7353 - val_acc: 0.6250\n",
      "Epoch 75/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4207 - acc: 0.8078 - val_loss: 0.7038 - val_acc: 0.6071\n",
      "Epoch 76/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4183 - acc: 0.7978 - val_loss: 0.7066 - val_acc: 0.6161\n",
      "Epoch 77/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3928 - acc: 0.8068 - val_loss: 0.7100 - val_acc: 0.6339\n",
      "Epoch 78/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4066 - acc: 0.7958 - val_loss: 0.7355 - val_acc: 0.6339\n",
      "Epoch 79/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4018 - acc: 0.7998 - val_loss: 0.7149 - val_acc: 0.6518\n",
      "Epoch 80/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4058 - acc: 0.8068 - val_loss: 0.7236 - val_acc: 0.6250\n",
      "Epoch 81/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4088 - acc: 0.8018 - val_loss: 0.7385 - val_acc: 0.6071\n",
      "Epoch 82/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4185 - acc: 0.7988 - val_loss: 0.6882 - val_acc: 0.6518\n",
      "Epoch 83/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3971 - acc: 0.8038 - val_loss: 0.7415 - val_acc: 0.6339\n",
      "Epoch 84/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3908 - acc: 0.8187 - val_loss: 0.7671 - val_acc: 0.5982\n",
      "Epoch 85/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4248 - acc: 0.8088 - val_loss: 0.7017 - val_acc: 0.6518\n",
      "Epoch 86/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3997 - acc: 0.8297 - val_loss: 0.7637 - val_acc: 0.6161\n",
      "Epoch 87/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3849 - acc: 0.8267 - val_loss: 0.7331 - val_acc: 0.6518\n",
      "Epoch 88/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3986 - acc: 0.8307 - val_loss: 0.7324 - val_acc: 0.6429\n",
      "Epoch 89/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3956 - acc: 0.8147 - val_loss: 0.7578 - val_acc: 0.6161\n",
      "Epoch 90/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4025 - acc: 0.8207 - val_loss: 0.7683 - val_acc: 0.6161\n",
      "Epoch 91/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3923 - acc: 0.8167 - val_loss: 0.7544 - val_acc: 0.6339\n",
      "Epoch 92/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.4014 - acc: 0.8048 - val_loss: 0.7421 - val_acc: 0.6607\n",
      "Epoch 93/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3823 - acc: 0.8327 - val_loss: 0.8136 - val_acc: 0.6607\n",
      "Epoch 94/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3908 - acc: 0.8167 - val_loss: 0.7784 - val_acc: 0.6786\n",
      "Epoch 95/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3783 - acc: 0.8247 - val_loss: 0.7691 - val_acc: 0.6429\n",
      "Epoch 96/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3794 - acc: 0.8217 - val_loss: 0.7671 - val_acc: 0.6786\n",
      "Epoch 97/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3780 - acc: 0.8357 - val_loss: 0.8971 - val_acc: 0.5982\n",
      "Epoch 98/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3835 - acc: 0.8177 - val_loss: 0.7550 - val_acc: 0.6964\n",
      "Epoch 99/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3973 - acc: 0.8078 - val_loss: 0.8141 - val_acc: 0.6607\n",
      "Epoch 100/100\n",
      "1004/1004 [==============================] - 1s 1ms/step - loss: 0.3668 - acc: 0.8127 - val_loss: 0.7545 - val_acc: 0.7054\n"
     ]
    }
   ],
   "source": [
    "nn_x2 = multi_x2.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y2 =  multi_y2.values\n",
    "nn_y2_train = to_categorical(nn_y2)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x2,nn_y2_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.71      0.82      2867\n",
      "    class 1       0.10      0.67      0.17       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.7046666666666667\n"
     ]
    }
   ],
   "source": [
    "predictions2_3 = model.predict(nn_val)\n",
    "pred2_3 = []\n",
    "for i in range(3000):\n",
    "    pred2_3.append(np.argmax(predictions2_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred2_3, target_names=target_names))\n",
    "\n",
    "val2_3_acc = metrics.accuracy_score(y_val,pred2_3)#验证集上的auc值\n",
    "print(val2_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.69      0.81      2867\n",
      "    class 1       0.10      0.74      0.18       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.6893333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions2 = predictions2_1+predictions2_2+pred2_3\n",
    "for i in range(3000):\n",
    "    if predictions2[i] >= 2:\n",
    "        predictions2[i] = 1\n",
    "    else:\n",
    "        predictions2[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions2, target_names=target_names))\n",
    "\n",
    "val2_acc = metrics.accuracy_score(y_val,predictions2)#验证集上的auc值\n",
    "print(val2_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8825  6118  3384 12926  1520 14680  6061  1634  6515  8082  1382 11534\n",
      "  3420  3984 10967 11457  6258  1414 14073  6671 11775  3159 14306 14630\n",
      "  8280  4823 12404 11967   761  7262  5792 14712  1623   396  6194 10859\n",
      "  1717  5278  5614  1862 13347 11846  7491 14134 11537  3290  2789 10792\n",
      " 13530  5271 11404  1091 11651 12812 13829  7208 11549  6345  1612 10122\n",
      "  7549  1938  3547  2208 11775  6548  8777  2146  7359 14501 14519  7694\n",
      " 12845 13143  3042  1268  9021 11313 12532  2094  4350  5535  1605 11357\n",
      "  8169 11147  5071  3064 14401  6177  1050  4174  2901  4122  7248  7696\n",
      "  7087  6829 11609  2318  2120 12565   137  5704  8964  7540   164  2137\n",
      "  9424 11933  6830 14773  2222 10727  1704 12492  7723  3059  5092  4847\n",
      " 13830   717 13816  2733  4464 11921 13348  3664 10210 12781 11980  8791\n",
      "  6977  6174 14519  7905 12895  6441  7223  8839 13765 13549 14600 14143\n",
      "  4816 11375  4444  9122 14587   794 10193  6419  5727   758  8476  6321\n",
      "  3473  3307 11279 13284  4031  5551 14511   287  2437 10059  9267  3462\n",
      "  7972  7571 12758 14539  6671   916 11675  7780  5288  7879 12408 11270\n",
      " 13524  8083  3727  3826  6755 11926  4063 11489   772 14598  4639 14340\n",
      "  4403  1368  3702   474 14127 10713  7930  6536  6370 14215  5816  2027\n",
      "  2884  1331  4860 13122 10221   856 11991 11054  1335  5021 11689 14635\n",
      "  1566  1362 11310 11074 12221  4236   905  7564 11009  6573  5604  3540\n",
      "  5600  8623  3678 13206  1620  4547 11352 11154  2805 13286 12040  1378\n",
      " 10152 10975  4712  9384  6310   243  2382  7742  7202  8740  9511 12666\n",
      " 11730   845  2514   219  3690  4134 14500  9517  5863  4374  3217  8161\n",
      " 12159  9330  3727  9219 13671  1759  6506 13432 11948 13715  6940   640\n",
      " 10965 10142  5513  5570 10225  9390  2450 11680  7637 10374  6754  3833\n",
      "  7798  7831  2761  8989 12955  6308  3655  3260  4706   575  1197 11290\n",
      "  4481 11076 12359  3399 10476 11593  3839  7025 13527  4196 10293 12022\n",
      "  1722 12633 10785  8176 12822  7859  3161 14574  6045  2596 14758   640\n",
      "  1707 14217 12486  2214 10415  5644  5934 12177 12190  5035 14778 14210\n",
      "  5352  5236  3580  1200  5658  7492  6696  5694  8629  2084  9018  4858\n",
      "  8325 11180 13029 12105  9303 12388  5259  3094  4227  7308  4400 11127\n",
      "   863 14315  1594  4481  8069  7381  1218  5292  8489  6892   665  1462\n",
      "  1076  4535  5103  7800    66 12137  5423 11063  2706  7019  3607 14220\n",
      "  3063  8079 13242  3932  7130  9376  9168 10535  4788  4400 10470  9067\n",
      "  1408 13206   432  1308  3354  8606   429  1476  2336  1262  9380  4554\n",
      "  3642 10189  9010 12769  3881  8827  1061 11910 12089 12440  5912  8255\n",
      " 11651  6338 14159  5341  8137  1640 11792  4077  1734 13018    24  7921\n",
      "  3373 11955  8554  9449  3854  5624  2046  5926 12419  3429 13329  1364\n",
      "  5034  7559  8620  2301  4365  9028 10556   810 12999  1432 11996  5153\n",
      " 12932 13917  3621 12773  7966 14398  8425  7934 11307 13127  5116  6203\n",
      "  5694  7560   425  4421  2914 12679 11138 11403  5270  6850 14700 10194\n",
      "  9061  6338  6264  4630 13701 11660 14217 10775  7436  5319  1702  6639\n",
      " 12913 13655 13794  1212  7159  4766     1  7047  5858   867 10676 14061\n",
      "  7519  2706  4456 11343  1313  6583  2984  8128  6468   964  8640 12577\n",
      "  2638  6359  1834  5443  9251 12256 12457  4816  4441  3109  3361 10058\n",
      "   187  7068 11048   987   318  3963 10867  4584  8910 12711  6561 10687\n",
      " 14213  6816  1770 13970  7514 10236  7431 12726  1155  2452  5675  3411\n",
      "  8184  2795 13748  5279 12706 14323  6752 10476]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train3 = resample_data(train_x1, 560)\n",
    "print(train3.shape)\n",
    "multi_x3, multi_y3 = concat_data(train3, train_x2)\n",
    "print(multi_x3.shape)\n",
    "print(multi_y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.81      2867\n",
      "    class 1       0.10      0.77      0.18       133\n",
      "\n",
      "avg / total       0.95      0.69      0.78      3000\n",
      "\n",
      "0.6853333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm3_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x3, multi_y3)\n",
    "predictions3_1 = gbm3_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions3_1, target_names=target_names))\n",
    "\n",
    "val3_1_acc = metrics.accuracy_score(y_val,predictions3_1)#验证集上的auc值\n",
    "print(val3_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.66      0.79      2867\n",
      "    class 1       0.10      0.80      0.17       133\n",
      "\n",
      "avg / total       0.95      0.67      0.76      3000\n",
      "\n",
      "0.666\n"
     ]
    }
   ],
   "source": [
    "clf3_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x3, multi_y3)\n",
    "predictions3_2 = clf3_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions3_2, target_names=target_names))\n",
    "\n",
    "val3_2_acc = metrics.accuracy_score(y_val,predictions3_2)#验证集上的auc值\n",
    "print(val3_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 4s 4ms/step - loss: 0.6810 - acc: 0.5417 - val_loss: 0.8006 - val_acc: 0.0893\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.6326 - acc: 0.6481 - val_loss: 4.9842e-04 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5853 - acc: 0.6869 - val_loss: 8.7624e-04 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5875 - acc: 0.6889 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5825 - acc: 0.6968 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5721 - acc: 0.6968 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5765 - acc: 0.6958 - val_loss: 0.0086 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5509 - acc: 0.7117 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5334 - acc: 0.7207 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5449 - acc: 0.7157 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5424 - acc: 0.7376 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5409 - acc: 0.7326 - val_loss: 0.0127 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5289 - acc: 0.7276 - val_loss: 0.0272 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5372 - acc: 0.7356 - val_loss: 0.0407 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5117 - acc: 0.7366 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5324 - acc: 0.7356 - val_loss: 0.2463 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5219 - acc: 0.7455 - val_loss: 0.2039 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5096 - acc: 0.7445 - val_loss: 0.4381 - val_acc: 0.8571\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5146 - acc: 0.7555 - val_loss: 0.2882 - val_acc: 0.9732\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5113 - acc: 0.7545 - val_loss: 0.3246 - val_acc: 0.9554\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5150 - acc: 0.7336 - val_loss: 0.3209 - val_acc: 0.9732\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5071 - acc: 0.7465 - val_loss: 0.3568 - val_acc: 0.9375\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5124 - acc: 0.7505 - val_loss: 0.4377 - val_acc: 0.8482\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5024 - acc: 0.7406 - val_loss: 0.4161 - val_acc: 0.9018\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5101 - acc: 0.7465 - val_loss: 0.4566 - val_acc: 0.8304\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5034 - acc: 0.7346 - val_loss: 0.4296 - val_acc: 0.8661\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4973 - acc: 0.7734 - val_loss: 0.4701 - val_acc: 0.8125\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5024 - acc: 0.7495 - val_loss: 0.4426 - val_acc: 0.8482\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5015 - acc: 0.7545 - val_loss: 0.5016 - val_acc: 0.8036\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4859 - acc: 0.7674 - val_loss: 0.4631 - val_acc: 0.8304\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4919 - acc: 0.7575 - val_loss: 0.4554 - val_acc: 0.8393\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4924 - acc: 0.7584 - val_loss: 0.4558 - val_acc: 0.8304\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4820 - acc: 0.7724 - val_loss: 0.5361 - val_acc: 0.7321\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4890 - acc: 0.7604 - val_loss: 0.6157 - val_acc: 0.6429\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4869 - acc: 0.7694 - val_loss: 0.5697 - val_acc: 0.6786\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4828 - acc: 0.7604 - val_loss: 0.5548 - val_acc: 0.6875\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4907 - acc: 0.7614 - val_loss: 0.5420 - val_acc: 0.7411\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4795 - acc: 0.7684 - val_loss: 0.5566 - val_acc: 0.6696\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4813 - acc: 0.7644 - val_loss: 0.5688 - val_acc: 0.6696\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 2s 2ms/step - loss: 0.4814 - acc: 0.7823 - val_loss: 0.6244 - val_acc: 0.6518\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4751 - acc: 0.7644 - val_loss: 0.5499 - val_acc: 0.7054\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4754 - acc: 0.7704 - val_loss: 0.5902 - val_acc: 0.7143\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.5026 - acc: 0.7624 - val_loss: 0.5992 - val_acc: 0.7143\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4683 - acc: 0.7734 - val_loss: 0.5518 - val_acc: 0.7411\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4704 - acc: 0.7704 - val_loss: 0.6897 - val_acc: 0.5804\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4851 - acc: 0.7694 - val_loss: 0.6146 - val_acc: 0.6429\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4727 - acc: 0.7753 - val_loss: 0.7070 - val_acc: 0.5714\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4778 - acc: 0.7674 - val_loss: 0.6353 - val_acc: 0.6607\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4552 - acc: 0.7763 - val_loss: 0.6793 - val_acc: 0.6071\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4697 - acc: 0.7704 - val_loss: 0.6610 - val_acc: 0.6071\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4740 - acc: 0.7763 - val_loss: 0.6223 - val_acc: 0.6518\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4689 - acc: 0.7763 - val_loss: 0.6416 - val_acc: 0.6339\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4839 - acc: 0.7763 - val_loss: 0.6944 - val_acc: 0.6161\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4589 - acc: 0.7793 - val_loss: 0.6514 - val_acc: 0.6786\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4571 - acc: 0.7813 - val_loss: 0.6229 - val_acc: 0.6875\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4428 - acc: 0.7873 - val_loss: 0.6824 - val_acc: 0.6250\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4756 - acc: 0.7783 - val_loss: 0.6091 - val_acc: 0.7232\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4678 - acc: 0.7813 - val_loss: 0.6381 - val_acc: 0.6786\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4536 - acc: 0.7773 - val_loss: 0.6637 - val_acc: 0.6696\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4661 - acc: 0.7753 - val_loss: 0.6587 - val_acc: 0.6429\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4459 - acc: 0.7873 - val_loss: 0.6381 - val_acc: 0.6875\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4431 - acc: 0.7813 - val_loss: 0.7016 - val_acc: 0.6339\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4596 - acc: 0.7773 - val_loss: 0.6372 - val_acc: 0.7143\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4565 - acc: 0.7744 - val_loss: 0.6326 - val_acc: 0.6964\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4486 - acc: 0.7873 - val_loss: 0.6425 - val_acc: 0.6786\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4508 - acc: 0.7744 - val_loss: 0.6324 - val_acc: 0.6964\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4435 - acc: 0.7863 - val_loss: 0.6774 - val_acc: 0.6518\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4386 - acc: 0.7982 - val_loss: 0.6983 - val_acc: 0.6518\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4465 - acc: 0.7813 - val_loss: 0.5778 - val_acc: 0.7411\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4606 - acc: 0.7594 - val_loss: 0.6774 - val_acc: 0.6518\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4472 - acc: 0.7932 - val_loss: 0.6636 - val_acc: 0.6607\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4532 - acc: 0.7843 - val_loss: 0.6588 - val_acc: 0.6607\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4490 - acc: 0.7763 - val_loss: 0.6328 - val_acc: 0.6964\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4440 - acc: 0.7883 - val_loss: 0.6618 - val_acc: 0.6607\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4371 - acc: 0.8032 - val_loss: 0.6506 - val_acc: 0.6875\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4547 - acc: 0.7763 - val_loss: 0.6744 - val_acc: 0.6696\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4431 - acc: 0.7853 - val_loss: 0.6447 - val_acc: 0.6875\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4399 - acc: 0.7873 - val_loss: 0.6915 - val_acc: 0.6161\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4337 - acc: 0.8012 - val_loss: 0.5790 - val_acc: 0.7411\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4361 - acc: 0.7962 - val_loss: 0.6880 - val_acc: 0.6518\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4200 - acc: 0.8052 - val_loss: 0.6970 - val_acc: 0.6250\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4555 - acc: 0.7972 - val_loss: 0.8031 - val_acc: 0.5625\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4457 - acc: 0.7942 - val_loss: 0.6796 - val_acc: 0.6518\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4401 - acc: 0.7992 - val_loss: 0.6904 - val_acc: 0.6429\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4178 - acc: 0.7992 - val_loss: 0.6658 - val_acc: 0.6696\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4166 - acc: 0.8012 - val_loss: 0.6274 - val_acc: 0.6875\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4327 - acc: 0.7922 - val_loss: 0.7225 - val_acc: 0.6339\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4252 - acc: 0.7982 - val_loss: 0.6229 - val_acc: 0.6875\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4115 - acc: 0.8211 - val_loss: 0.6929 - val_acc: 0.6518\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4253 - acc: 0.8121 - val_loss: 0.6376 - val_acc: 0.6786\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4273 - acc: 0.8062 - val_loss: 0.6537 - val_acc: 0.6786\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4477 - acc: 0.7813 - val_loss: 0.6920 - val_acc: 0.6339\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4197 - acc: 0.8012 - val_loss: 0.6957 - val_acc: 0.6339\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4254 - acc: 0.8091 - val_loss: 0.6779 - val_acc: 0.6429\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4162 - acc: 0.8052 - val_loss: 0.7060 - val_acc: 0.6071\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.3984 - acc: 0.8131 - val_loss: 0.6571 - val_acc: 0.6964\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4242 - acc: 0.8141 - val_loss: 0.7401 - val_acc: 0.6250\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4182 - acc: 0.8022 - val_loss: 0.6611 - val_acc: 0.6518\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4132 - acc: 0.7932 - val_loss: 0.7541 - val_acc: 0.6429\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 1ms/step - loss: 0.4278 - acc: 0.8072 - val_loss: 0.6611 - val_acc: 0.6607\n"
     ]
    }
   ],
   "source": [
    "nn_x3 = multi_x3.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y3 =  multi_y3.values\n",
    "nn_y3_train = to_categorical(nn_y3)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x3,nn_y3_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.72      0.83      2867\n",
      "    class 1       0.09      0.62      0.16       133\n",
      "\n",
      "avg / total       0.94      0.71      0.80      3000\n",
      "\n",
      "0.7113333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions3_3 = model.predict(nn_val)\n",
    "pred3_3 = []\n",
    "for i in range(3000):\n",
    "    pred3_3.append(np.argmax(predictions3_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred3_3, target_names=target_names))\n",
    "\n",
    "val3_3_acc = metrics.accuracy_score(y_val,pred3_3)#验证集上的auc值\n",
    "print(val3_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.82      2867\n",
      "    class 1       0.10      0.72      0.18       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "predictions3 = predictions3_1+predictions3_2+pred3_3\n",
    "for i in range(3000):\n",
    "    if predictions3[i] >= 2:\n",
    "        predictions3[i] = 1\n",
    "    else:\n",
    "        predictions3[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions3, target_names=target_names))\n",
    "\n",
    "val3_acc = metrics.accuracy_score(y_val,predictions3)#验证集上的auc值\n",
    "print(val3_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  273 12780  1191 14105   326  3987  4498  4209  1191  9339  6255  5394\n",
      "   906  2472  1333 12888  2772   366   524  9115  6115 12977 14349 14420\n",
      " 14627  7415  3744  6844  1154  9505 13389  8751  5669  8607 13072  7165\n",
      " 10378 12428  6135  8252 11119  7204  6984  7799  4457 14005    78  2340\n",
      "  5679  9340  7452 14538 14519 12234  1544  7673  6161  2446 14261  3615\n",
      "  2003 10109   908  2484 12478  9188  2663  6716  9391 13475 10661  7011\n",
      "  5910 14676   693   593  3798 10543  2780   275  6627  2486  3486  6703\n",
      "  4245  6010 10429   476 13852 10602  4757 11202 11454  8250  7162  2722\n",
      "  8442  7007  1407  3314  8496  2072  7641  1677  7852  7736  4563  5244\n",
      "  7354 13996 10324  4733  8195  8512 14584  2723 12854 14611  8620  1134\n",
      "  2596  1249  3700  6606 12202  6124  2055 12366 14118 13640  4659  3381\n",
      "  6412  7117  1925   363  5773  2320  5132  7366  4584 11079  6028  1836\n",
      "  7043  5332 12991  3697 11666 11819 12254  3131 13729  2148  1920  7807\n",
      "  1007  7440  7665  5338 12359 12498  3511 13479   470  6542  8760  4131\n",
      "     1  4350 10113 13247 14339  4584  2156  2956  4134  4037 11719 10701\n",
      "   632  1385    42 13994  3674  1179  1324  6893 13035   965  5513 14659\n",
      "  6301  8235   483  8412  6121   287   191 12583  8487  9111  6809 13991\n",
      " 10193 10299   886  7487 12553 11183  4414  7505  8566  4192  8796 13734\n",
      " 12654  4658  5822  4879 10134  5769  8184 12509  2243  2292  2804 10818\n",
      " 14008  5763  9202 12706  4519 12527 13969  8775  1885   169  3081 11345\n",
      "  6103 13401 10331  4253 13638 14306  1676  1885 10787   544   481 13525\n",
      "  7729  2982 13608  3338  2502 12432 12411 14254  1022 10801  3485 11550\n",
      " 11522  7711  3222  1213 10074 10435  2597  1545  4044  5394 10947  5054\n",
      " 14359  9084  4168 11567  3547  1384 10695  1568  8250  6082  5307 11207\n",
      " 12871 12385  2073 13348  5874   881  3361 13075  2205 10478  7238  5700\n",
      "  6731 10882 11262  7825  2235  7911  6902  4734  9501  7601  6966 10730\n",
      "  2256 11441 11852  8908  6780  7079 12258  3984 11347 10033  1101 14543\n",
      " 12394  2435 13680 13631 11166  5586  2743 14390  4590 13358  1447  9549\n",
      " 11091 12585  7119 13798  5466  7001  7314  4547  4475 10265  8327  5172\n",
      " 13995  6798  6581 12874  6271 11354 10443 14797 11280  2841   858  2075\n",
      " 14397 14433  6670  2758 11684  1969  2034  5917   853 12996  3764  6055\n",
      "  4873 11524  5524  2901   710  6231 10045  9268  3902  6861  1431  1175\n",
      "    88  5307  1394  8811   285 14045  4134   753 13033 11807 14506  7739\n",
      " 14311   321  6561 11997  4041  2389  3347  3806  2992   263 13786  5949\n",
      "  8079 13835  1396  6206 11836  6302  1408  5938  4715  9140 11329  9105\n",
      " 10256  1251  7164  2527  5051  8367 12352 13387  7714  1215 11924  1566\n",
      "   314  1489  9479  7016  7352 13624   238  7394  5723  5351  8998 11935\n",
      " 10799  3287 11216  1473 12579  9300 10809  2120  7984   503  8429 13119\n",
      " 14676   916  5247  6203 13117  8564  8134  3121 12687  5988  8457 13938\n",
      "  3755  5964  1382  1139  4472  4438 12716  3920  5882 12385 12610 12247\n",
      " 10806 10993  9203  3076  8794  1532 12737 10468   475  1477  7701 10750\n",
      "  1218  3677 10875  3283  2457 14820   331 13618   881    19  8196 13383\n",
      " 11206  1712  8555  5032  8484 11597  8479  8212  7515  8650  9187 11009\n",
      "  6836  4713  4874  3802   531  6569 14017  7841  5819  7503 10567  2984\n",
      "  8213  6462  8738  6441  3001  9127  8055  1847  5938  8639  3792  9079\n",
      " 11362 10113 11474  6664  8159  3942  2594  6210  7703  2728  4509 14385\n",
      " 12135  8986 13405   165 10686  2501 13853  9254]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train4 = resample_data(train_x1, 560)\n",
    "print(train4.shape)\n",
    "multi_x4, multi_y4 = concat_data(train4, train_x2)\n",
    "print(multi_x4.shape)\n",
    "print(multi_y4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.69      0.81      2867\n",
      "    class 1       0.11      0.78      0.19       133\n",
      "\n",
      "avg / total       0.95      0.70      0.79      3000\n",
      "\n",
      "0.6963333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm4_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x4, multi_y4)\n",
    "predictions4_1 = gbm4_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions4_1, target_names=target_names))\n",
    "\n",
    "val4_1_acc = metrics.accuracy_score(y_val,predictions4_1)#验证集上的auc值\n",
    "print(val4_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.69      0.81      2867\n",
      "    class 1       0.10      0.78      0.18       133\n",
      "\n",
      "avg / total       0.95      0.69      0.78      3000\n",
      "\n",
      "0.6896666666666667\n"
     ]
    }
   ],
   "source": [
    "clf4_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x4, multi_y4)\n",
    "predictions4_2 = clf4_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions4_2, target_names=target_names))\n",
    "\n",
    "val4_2_acc = metrics.accuracy_score(y_val,predictions4_2)#验证集上的auc值\n",
    "print(val4_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 4s 4ms/step - loss: 0.6817 - acc: 0.5477 - val_loss: 2.2650 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 791us/step - loss: 0.6462 - acc: 0.6103 - val_loss: 0.9260 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 810us/step - loss: 0.6104 - acc: 0.6521 - val_loss: 1.2995 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.5913 - acc: 0.6849 - val_loss: 0.8307 - val_acc: 0.2054\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 786us/step - loss: 0.5863 - acc: 0.6849 - val_loss: 0.5586 - val_acc: 0.8393\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.5789 - acc: 0.6869 - val_loss: 0.4524 - val_acc: 0.9554\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 786us/step - loss: 0.5769 - acc: 0.6799 - val_loss: 0.8372 - val_acc: 0.2321\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.5730 - acc: 0.7028 - val_loss: 0.7581 - val_acc: 0.4107\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.5569 - acc: 0.7097 - val_loss: 0.8176 - val_acc: 0.2857\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 793us/step - loss: 0.5544 - acc: 0.7018 - val_loss: 0.6481 - val_acc: 0.6518\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 818us/step - loss: 0.5542 - acc: 0.7038 - val_loss: 0.2132 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 818us/step - loss: 0.5639 - acc: 0.6978 - val_loss: 0.8075 - val_acc: 0.3125\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 780us/step - loss: 0.5444 - acc: 0.7157 - val_loss: 0.6142 - val_acc: 0.7143\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.5521 - acc: 0.7197 - val_loss: 0.9098 - val_acc: 0.1786\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 789us/step - loss: 0.5564 - acc: 0.7117 - val_loss: 0.7935 - val_acc: 0.3304\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 772us/step - loss: 0.5403 - acc: 0.7266 - val_loss: 0.7716 - val_acc: 0.3750\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 771us/step - loss: 0.5447 - acc: 0.7187 - val_loss: 0.7659 - val_acc: 0.3750\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 759us/step - loss: 0.5393 - acc: 0.7227 - val_loss: 0.7388 - val_acc: 0.4375\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 808us/step - loss: 0.5433 - acc: 0.7227 - val_loss: 0.5365 - val_acc: 0.8661\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 895us/step - loss: 0.5383 - acc: 0.7207 - val_loss: 0.5225 - val_acc: 0.8214\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 844us/step - loss: 0.5292 - acc: 0.7197 - val_loss: 0.4364 - val_acc: 0.9464\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 794us/step - loss: 0.5147 - acc: 0.7247 - val_loss: 0.4228 - val_acc: 0.9286\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 806us/step - loss: 0.5352 - acc: 0.7207 - val_loss: 0.4397 - val_acc: 0.9286\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 828us/step - loss: 0.5334 - acc: 0.7356 - val_loss: 0.5799 - val_acc: 0.6875\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 809us/step - loss: 0.5152 - acc: 0.7356 - val_loss: 0.5728 - val_acc: 0.6964\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 844us/step - loss: 0.5325 - acc: 0.7326 - val_loss: 0.6799 - val_acc: 0.5357\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 813us/step - loss: 0.5320 - acc: 0.7107 - val_loss: 0.5830 - val_acc: 0.6786\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 763us/step - loss: 0.5156 - acc: 0.7356 - val_loss: 0.5363 - val_acc: 0.7143\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.5114 - acc: 0.7416 - val_loss: 0.5723 - val_acc: 0.6786\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5287 - acc: 0.7256 - val_loss: 0.5809 - val_acc: 0.6964\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5111 - acc: 0.7386 - val_loss: 0.5177 - val_acc: 0.7500\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.5130 - acc: 0.7376 - val_loss: 0.5853 - val_acc: 0.6696\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 750us/step - loss: 0.5270 - acc: 0.7346 - val_loss: 0.5204 - val_acc: 0.7500\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.5154 - acc: 0.7346 - val_loss: 0.5703 - val_acc: 0.6875\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.4985 - acc: 0.7406 - val_loss: 0.5766 - val_acc: 0.6875\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 733us/step - loss: 0.5293 - acc: 0.7286 - val_loss: 0.5966 - val_acc: 0.6696\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.5111 - acc: 0.7366 - val_loss: 0.5630 - val_acc: 0.7054\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.5147 - acc: 0.7515 - val_loss: 0.5788 - val_acc: 0.6696\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 759us/step - loss: 0.5211 - acc: 0.7276 - val_loss: 0.5754 - val_acc: 0.7054\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.5056 - acc: 0.7306 - val_loss: 0.5754 - val_acc: 0.6964\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.5149 - acc: 0.7455 - val_loss: 0.5856 - val_acc: 0.6964\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 741us/step - loss: 0.5057 - acc: 0.7495 - val_loss: 0.5808 - val_acc: 0.7143\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5204 - acc: 0.7386 - val_loss: 0.5682 - val_acc: 0.7143\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 744us/step - loss: 0.5098 - acc: 0.7465 - val_loss: 0.6286 - val_acc: 0.6250\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.5035 - acc: 0.7465 - val_loss: 0.5645 - val_acc: 0.7143\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5177 - acc: 0.7465 - val_loss: 0.6200 - val_acc: 0.6250\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 752us/step - loss: 0.4995 - acc: 0.7495 - val_loss: 0.5699 - val_acc: 0.7143\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 758us/step - loss: 0.4957 - acc: 0.7475 - val_loss: 0.5727 - val_acc: 0.7321\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 765us/step - loss: 0.5109 - acc: 0.7535 - val_loss: 0.5849 - val_acc: 0.6607\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5036 - acc: 0.7416 - val_loss: 0.6437 - val_acc: 0.6071\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 771us/step - loss: 0.4951 - acc: 0.7455 - val_loss: 0.5947 - val_acc: 0.6518\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 762us/step - loss: 0.5041 - acc: 0.7624 - val_loss: 0.5944 - val_acc: 0.6786\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.4887 - acc: 0.7604 - val_loss: 0.6088 - val_acc: 0.6429\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.5019 - acc: 0.7416 - val_loss: 0.6501 - val_acc: 0.6161\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.4960 - acc: 0.7356 - val_loss: 0.6452 - val_acc: 0.6250\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.4964 - acc: 0.7485 - val_loss: 0.5975 - val_acc: 0.6607\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 780us/step - loss: 0.4971 - acc: 0.7455 - val_loss: 0.6391 - val_acc: 0.6161\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 754us/step - loss: 0.4981 - acc: 0.7505 - val_loss: 0.6111 - val_acc: 0.6339\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 766us/step - loss: 0.4764 - acc: 0.7525 - val_loss: 0.6264 - val_acc: 0.6161\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.4829 - acc: 0.7565 - val_loss: 0.6099 - val_acc: 0.6250\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 753us/step - loss: 0.4851 - acc: 0.7614 - val_loss: 0.6736 - val_acc: 0.5982\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.4855 - acc: 0.7535 - val_loss: 0.6372 - val_acc: 0.6250\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 884us/step - loss: 0.5055 - acc: 0.7336 - val_loss: 0.6203 - val_acc: 0.6161\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 812us/step - loss: 0.5020 - acc: 0.7555 - val_loss: 0.6114 - val_acc: 0.6696\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 834us/step - loss: 0.4978 - acc: 0.7624 - val_loss: 0.6208 - val_acc: 0.6429\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.5017 - acc: 0.7435 - val_loss: 0.6121 - val_acc: 0.6607\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 879us/step - loss: 0.4807 - acc: 0.7694 - val_loss: 0.6336 - val_acc: 0.6607\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 860us/step - loss: 0.4842 - acc: 0.7535 - val_loss: 0.6446 - val_acc: 0.6607\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.4705 - acc: 0.7773 - val_loss: 0.6530 - val_acc: 0.6071\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 895us/step - loss: 0.4843 - acc: 0.7644 - val_loss: 0.6665 - val_acc: 0.5714\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 760us/step - loss: 0.4808 - acc: 0.7634 - val_loss: 0.6017 - val_acc: 0.6518\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 750us/step - loss: 0.4828 - acc: 0.7664 - val_loss: 0.6518 - val_acc: 0.6250\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 773us/step - loss: 0.4987 - acc: 0.7465 - val_loss: 0.6135 - val_acc: 0.6696\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 810us/step - loss: 0.4841 - acc: 0.7535 - val_loss: 0.6322 - val_acc: 0.6339\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 807us/step - loss: 0.4646 - acc: 0.7684 - val_loss: 0.6389 - val_acc: 0.6429\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 774us/step - loss: 0.4856 - acc: 0.7525 - val_loss: 0.6392 - val_acc: 0.5982\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 816us/step - loss: 0.4696 - acc: 0.7803 - val_loss: 0.6573 - val_acc: 0.6071\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 815us/step - loss: 0.4774 - acc: 0.7584 - val_loss: 0.6174 - val_acc: 0.6696\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.4662 - acc: 0.7843 - val_loss: 0.6297 - val_acc: 0.6518\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 753us/step - loss: 0.4831 - acc: 0.7545 - val_loss: 0.6024 - val_acc: 0.6696\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 719us/step - loss: 0.4980 - acc: 0.7654 - val_loss: 0.6162 - val_acc: 0.6607\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4613 - acc: 0.7753 - val_loss: 0.6451 - val_acc: 0.6161\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4651 - acc: 0.7674 - val_loss: 0.6125 - val_acc: 0.6429\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 721us/step - loss: 0.4628 - acc: 0.7704 - val_loss: 0.6074 - val_acc: 0.6339\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 831us/step - loss: 0.4845 - acc: 0.7624 - val_loss: 0.5900 - val_acc: 0.6607\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 808us/step - loss: 0.4734 - acc: 0.7575 - val_loss: 0.6186 - val_acc: 0.6339\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.4471 - acc: 0.7714 - val_loss: 0.6280 - val_acc: 0.6250\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.4669 - acc: 0.7714 - val_loss: 0.6552 - val_acc: 0.5982\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.4765 - acc: 0.7704 - val_loss: 0.5987 - val_acc: 0.7054\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4678 - acc: 0.7644 - val_loss: 0.6379 - val_acc: 0.6518\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 730us/step - loss: 0.4530 - acc: 0.7724 - val_loss: 0.6421 - val_acc: 0.6429\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4753 - acc: 0.7714 - val_loss: 0.6692 - val_acc: 0.5982\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.4721 - acc: 0.7614 - val_loss: 0.6591 - val_acc: 0.6071\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4631 - acc: 0.7744 - val_loss: 0.6155 - val_acc: 0.6696\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4533 - acc: 0.7724 - val_loss: 0.6435 - val_acc: 0.6607\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4520 - acc: 0.7773 - val_loss: 0.6652 - val_acc: 0.6339\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.4604 - acc: 0.7793 - val_loss: 0.6599 - val_acc: 0.6071\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4744 - acc: 0.7744 - val_loss: 0.6228 - val_acc: 0.6429\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4550 - acc: 0.7843 - val_loss: 0.6918 - val_acc: 0.5804\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 718us/step - loss: 0.4549 - acc: 0.7624 - val_loss: 0.6800 - val_acc: 0.5893\n"
     ]
    }
   ],
   "source": [
    "nn_x4 = multi_x4.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y4 =  multi_y4.values\n",
    "nn_y4_train = to_categorical(nn_y4)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x4,nn_y4_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.74      0.84      2867\n",
      "    class 1       0.10      0.64      0.18       133\n",
      "\n",
      "avg / total       0.94      0.73      0.81      3000\n",
      "\n",
      "0.7343333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions4_3 = model.predict(nn_val)\n",
    "pred4_3 = []\n",
    "for i in range(3000):\n",
    "    pred4_3.append(np.argmax(predictions4_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred4_3, target_names=target_names))\n",
    "\n",
    "val4_3_acc = metrics.accuracy_score(y_val,pred4_3)#验证集上的auc值\n",
    "print(val4_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.71      0.82      2867\n",
      "    class 1       0.11      0.79      0.19       133\n",
      "\n",
      "avg / total       0.95      0.71      0.80      3000\n",
      "\n",
      "0.71\n"
     ]
    }
   ],
   "source": [
    "predictions4 = predictions4_1+predictions4_2+pred4_3\n",
    "for i in range(3000):\n",
    "    if predictions4[i] >= 2:\n",
    "        predictions4[i] = 1\n",
    "    else:\n",
    "        predictions4[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions4, target_names=target_names))\n",
    "\n",
    "val4_acc = metrics.accuracy_score(y_val,predictions4)#验证集上的auc值\n",
    "print(val4_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9267 10117   498 13761  7834  8948 14529  7875  8398  4746  8973 14676\n",
      " 13241  8544 10934  9205  3310  4896 11252  8178 14477  5926  6972 10207\n",
      "  5255  2473  4471 11431  2568 13039  9435  3965  7719 13597  7087 12520\n",
      " 13544  8237  7638 13482  2620  6699  5475  7339  6452  6807   517  5756\n",
      " 13669 11536 12408  2227  2469 11357  5179  9265  7513   985  7833   482\n",
      "  3243  7854   498  2121  7409 10732  6702  6090  3311   296   462  6661\n",
      "  2086  7169  1672  8896  8698 11773   361   638 14341 14339   405 12344\n",
      "  2563 14340 12841   578  4255   464   850  7296  5941 12760  8669   807\n",
      "  6021  5472 12728 12202  3130  1688  4429 13949 14741 14037  1534  5038\n",
      " 14458 14051  8306  4421 11052  5263 11719 11353  1590  5066  2770  8673\n",
      "  7668  8844  8039  7856  8677  6779 12799  8119    78 11212 10246  1745\n",
      "  3285  5809  4193 11164 12136  6828 12111  6897 12120 10331  5988  9309\n",
      "  1825  8753 12861  5443 12495 10394  2005  6676 14034 10499  1864  7082\n",
      " 12428 11090 13361  5249   829  2193 13566  6873 14174  3007  5396 13190\n",
      "  5786  6010  9120 12102   492 12531 13539 10685  2374 10770 13351   924\n",
      "  4154  9496  3912  8263 11844  1756  8337 10606  1669  3590  5034  2458\n",
      "   155  7046 11431 10502 13091 10717  4331 10898  8471 13625 10006  7594\n",
      "  7711  5750  2787  5394 10451  1049  5092  9384  8632 13475 11189  2773\n",
      "   298  2450  5874  1098  2397  8158   317   119  8216  3870  7783  3031\n",
      "  2171 12793  4042  3026  2209 10091  9061 12599 14879 13140  5926  1067\n",
      " 12206  4498 11485  1912 13080    69 13390  2195 10699 14836    17 11803\n",
      "  2401  3716 13885  8487   761  5923  7285  1955  9172  5923  4459  3852\n",
      "  9120  3373  9483 13225  5345  3648  3358   311  7336  4490  1137 10212\n",
      "  8359  3781 11266  9117  4806  4394   910  7844  1506  6453  6545  7905\n",
      "   439  7732  7672   256  3732 12421 11793  5345  2153  5358 10619 11805\n",
      " 14145  4776  3695  8358  2348  7598 11291  3237  8022 13426  3705  5744\n",
      " 12684   917  9108  6292  2315 12306 12938  5143 13955 10187 13035 10118\n",
      " 14803 14027   778  4101  9435   983 11286  1485  4475 14258  9167  4415\n",
      " 14161  7347  3807 10051  6946  5345  3336  1241  2402  6091  8311  7229\n",
      "  5552 12338 10161 10621 13345  1103  3783 14757   888    93 11007 12323\n",
      "  7809  3096  8222  2969  5292  6292 13543  2334   151 12850  2573   803\n",
      "  2724  7150  4634  6161  4469  6565  4002  4848   385  4736  4902 12595\n",
      " 14802  6537  5526 13472  8345 10684  1350  9207  7625 13023  6251 10094\n",
      " 12706  9514  9521 13459  2616 13317  2479 13375  7841 13494  2450  1610\n",
      "  8304 10494  3556 12094  3700  5792  2205  6481 12376 13171  7068  7107\n",
      " 14334 10838  7541    33  4134  3169  3900 11560  8888 10783 10768  4055\n",
      "  3685  7717 12970  3542  7883 13032  3952  1819  4706  1645 14028  3747\n",
      "  2934  2558 13351  6564 11880 13564 10283  3286 13216 10414  3747  1825\n",
      "  8233  7263  8496 10887  6320 14085  4084   774 10340  1721  4210 11056\n",
      "  3329 12471  3877  6587  4070  7085  4447 11015 13734  7969 13445  4581\n",
      "  5361  5284  8636 12859  2764 11938   700  1421 13555 14575  2540 11527\n",
      " 12490  2327  1987  6258 14556 14508  8632 13044  5441  5522 13204  5067\n",
      " 13719  8176  8639 12176  2927  7712 13006  5808  6111  4848 10492 13406\n",
      " 14188  1013  1175  7698  7404   733 11853  7785  6082 10521 14504 10815\n",
      " 13480 11843  4904  1760 13798  4237  5768  6004  8964 12160 13001  4273\n",
      " 13805  1344  3281  5582  5959 13825 13249 10625  7918  5904 12510 11647\n",
      " 12460 12419  4744 12610  5868  4063 13751  9218]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train5 = resample_data(train_x1, 560)\n",
    "print(train5.shape)\n",
    "multi_x5, multi_y5 = concat_data(train5, train_x2)\n",
    "print(multi_x5.shape)\n",
    "print(multi_y5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.81      2867\n",
      "    class 1       0.10      0.73      0.17       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.6866666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm5_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x5, multi_y5)\n",
    "predictions5_1 = gbm5_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions5_1, target_names=target_names))\n",
    "\n",
    "val5_1_acc = metrics.accuracy_score(y_val,predictions5_1)#验证集上的auc值\n",
    "print(val5_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.80      2867\n",
      "    class 1       0.09      0.71      0.17       133\n",
      "\n",
      "avg / total       0.94      0.68      0.78      3000\n",
      "\n",
      "0.683\n"
     ]
    }
   ],
   "source": [
    "clf5_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x5, multi_y5)\n",
    "predictions5_2 = clf5_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions5_2, target_names=target_names))\n",
    "\n",
    "val5_2_acc = metrics.accuracy_score(y_val,predictions5_2)#验证集上的auc值\n",
    "print(val5_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 4s 4ms/step - loss: 0.6845 - acc: 0.5517 - val_loss: 2.6541 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.6547 - acc: 0.5765 - val_loss: 0.5389 - val_acc: 0.8304\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 810us/step - loss: 0.6221 - acc: 0.6660 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 770us/step - loss: 0.6127 - acc: 0.6511 - val_loss: 0.0321 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 824us/step - loss: 0.5834 - acc: 0.6819 - val_loss: 0.0477 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 792us/step - loss: 0.5972 - acc: 0.6740 - val_loss: 0.0682 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 808us/step - loss: 0.5745 - acc: 0.6879 - val_loss: 0.1390 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 770us/step - loss: 0.5746 - acc: 0.6879 - val_loss: 0.0336 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.5610 - acc: 0.7008 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.5796 - acc: 0.6879 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.5676 - acc: 0.6988 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 798us/step - loss: 0.5686 - acc: 0.6948 - val_loss: 0.0287 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 813us/step - loss: 0.5717 - acc: 0.7018 - val_loss: 0.0906 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 802us/step - loss: 0.5689 - acc: 0.7028 - val_loss: 0.0667 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 837us/step - loss: 0.5571 - acc: 0.7187 - val_loss: 0.1231 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 800us/step - loss: 0.5511 - acc: 0.7217 - val_loss: 0.1335 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 792us/step - loss: 0.5555 - acc: 0.7187 - val_loss: 0.1153 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 794us/step - loss: 0.5417 - acc: 0.7137 - val_loss: 0.0882 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 758us/step - loss: 0.5522 - acc: 0.7177 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.5404 - acc: 0.7326 - val_loss: 0.1705 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 777us/step - loss: 0.5423 - acc: 0.7127 - val_loss: 0.2518 - val_acc: 0.9911\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5419 - acc: 0.7286 - val_loss: 0.2339 - val_acc: 0.9911\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 758us/step - loss: 0.5367 - acc: 0.7266 - val_loss: 0.2827 - val_acc: 0.9464\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5427 - acc: 0.7217 - val_loss: 0.2416 - val_acc: 0.9911\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.5269 - acc: 0.7306 - val_loss: 0.3031 - val_acc: 0.9375\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5273 - acc: 0.7316 - val_loss: 0.3164 - val_acc: 0.9107\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 734us/step - loss: 0.5396 - acc: 0.7117 - val_loss: 0.4363 - val_acc: 0.8036\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.5270 - acc: 0.7495 - val_loss: 0.4381 - val_acc: 0.8036\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.5298 - acc: 0.7306 - val_loss: 0.4534 - val_acc: 0.7946\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5230 - acc: 0.7396 - val_loss: 0.4401 - val_acc: 0.7946\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.5311 - acc: 0.7247 - val_loss: 0.4547 - val_acc: 0.7946\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.5422 - acc: 0.7097 - val_loss: 0.4586 - val_acc: 0.7946\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 735us/step - loss: 0.5390 - acc: 0.7256 - val_loss: 0.5071 - val_acc: 0.7500\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.5304 - acc: 0.7286 - val_loss: 0.5242 - val_acc: 0.7143\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.5297 - acc: 0.7406 - val_loss: 0.4685 - val_acc: 0.7857\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 777us/step - loss: 0.5333 - acc: 0.7376 - val_loss: 0.4842 - val_acc: 0.7500\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 734us/step - loss: 0.5220 - acc: 0.7445 - val_loss: 0.5338 - val_acc: 0.7054\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 741us/step - loss: 0.5189 - acc: 0.7425 - val_loss: 0.5346 - val_acc: 0.6607\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5262 - acc: 0.7376 - val_loss: 0.5214 - val_acc: 0.6875\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 1s 735us/step - loss: 0.5467 - acc: 0.7207 - val_loss: 0.5405 - val_acc: 0.6696\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5300 - acc: 0.7376 - val_loss: 0.5790 - val_acc: 0.6339\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.5188 - acc: 0.7256 - val_loss: 0.5729 - val_acc: 0.6250\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5302 - acc: 0.7465 - val_loss: 0.6133 - val_acc: 0.6250\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 744us/step - loss: 0.5166 - acc: 0.7455 - val_loss: 0.6592 - val_acc: 0.6161\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5260 - acc: 0.7336 - val_loss: 0.6042 - val_acc: 0.6161\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.5176 - acc: 0.7356 - val_loss: 0.5678 - val_acc: 0.6250\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5074 - acc: 0.7515 - val_loss: 0.6702 - val_acc: 0.6071\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 760us/step - loss: 0.4972 - acc: 0.7495 - val_loss: 0.6382 - val_acc: 0.6161\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.5032 - acc: 0.7356 - val_loss: 0.6700 - val_acc: 0.5982\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 752us/step - loss: 0.4988 - acc: 0.7485 - val_loss: 0.6838 - val_acc: 0.5982\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5055 - acc: 0.7575 - val_loss: 0.6756 - val_acc: 0.5982\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.5115 - acc: 0.7386 - val_loss: 0.6652 - val_acc: 0.6071\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5085 - acc: 0.7535 - val_loss: 0.6273 - val_acc: 0.6161\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.5023 - acc: 0.7445 - val_loss: 0.6944 - val_acc: 0.5982\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 760us/step - loss: 0.5068 - acc: 0.7425 - val_loss: 0.6464 - val_acc: 0.6071\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 776us/step - loss: 0.4876 - acc: 0.7584 - val_loss: 0.6469 - val_acc: 0.6071\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.4990 - acc: 0.7555 - val_loss: 0.6751 - val_acc: 0.6071\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 760us/step - loss: 0.5075 - acc: 0.7445 - val_loss: 0.6372 - val_acc: 0.6161\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.5078 - acc: 0.7406 - val_loss: 0.6379 - val_acc: 0.6161\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.4859 - acc: 0.7694 - val_loss: 0.6784 - val_acc: 0.5982\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 757us/step - loss: 0.4954 - acc: 0.7465 - val_loss: 0.6863 - val_acc: 0.5982\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 744us/step - loss: 0.5171 - acc: 0.7386 - val_loss: 0.6380 - val_acc: 0.6071\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.5131 - acc: 0.7416 - val_loss: 0.6894 - val_acc: 0.5893\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 792us/step - loss: 0.4900 - acc: 0.7614 - val_loss: 0.6326 - val_acc: 0.6071\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 837us/step - loss: 0.4854 - acc: 0.7535 - val_loss: 0.6887 - val_acc: 0.5982\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.4958 - acc: 0.7465 - val_loss: 0.6208 - val_acc: 0.6161\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 754us/step - loss: 0.4921 - acc: 0.7455 - val_loss: 0.6263 - val_acc: 0.6071\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 797us/step - loss: 0.5003 - acc: 0.7485 - val_loss: 0.6771 - val_acc: 0.5982\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 858us/step - loss: 0.4908 - acc: 0.7495 - val_loss: 0.6257 - val_acc: 0.6250\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 782us/step - loss: 0.4973 - acc: 0.7525 - val_loss: 0.6516 - val_acc: 0.5982\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.4808 - acc: 0.7594 - val_loss: 0.6857 - val_acc: 0.5893\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 766us/step - loss: 0.4929 - acc: 0.7465 - val_loss: 0.6703 - val_acc: 0.5982\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.4861 - acc: 0.7654 - val_loss: 0.6625 - val_acc: 0.6071\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4864 - acc: 0.7565 - val_loss: 0.6589 - val_acc: 0.6161\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 719us/step - loss: 0.4848 - acc: 0.7584 - val_loss: 0.6716 - val_acc: 0.6161\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4871 - acc: 0.7664 - val_loss: 0.6632 - val_acc: 0.6161\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.4829 - acc: 0.7594 - val_loss: 0.6815 - val_acc: 0.6071\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 774us/step - loss: 0.4968 - acc: 0.7555 - val_loss: 0.6690 - val_acc: 0.5982\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 800us/step - loss: 0.4871 - acc: 0.7664 - val_loss: 0.6570 - val_acc: 0.5982\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 776us/step - loss: 0.4855 - acc: 0.7515 - val_loss: 0.7650 - val_acc: 0.5625\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 726us/step - loss: 0.5020 - acc: 0.7455 - val_loss: 0.6927 - val_acc: 0.5804\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 800us/step - loss: 0.4829 - acc: 0.7485 - val_loss: 0.6944 - val_acc: 0.5982\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 795us/step - loss: 0.4766 - acc: 0.7555 - val_loss: 0.6438 - val_acc: 0.6518\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 779us/step - loss: 0.4732 - acc: 0.7684 - val_loss: 0.7084 - val_acc: 0.5982\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.4738 - acc: 0.7684 - val_loss: 0.6335 - val_acc: 0.6339\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.4884 - acc: 0.7515 - val_loss: 0.6728 - val_acc: 0.6250\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.4874 - acc: 0.7565 - val_loss: 0.6792 - val_acc: 0.6250\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 720us/step - loss: 0.4639 - acc: 0.7734 - val_loss: 0.6840 - val_acc: 0.5982\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4762 - acc: 0.7565 - val_loss: 0.6707 - val_acc: 0.6071\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4857 - acc: 0.7674 - val_loss: 0.6726 - val_acc: 0.5893\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 772us/step - loss: 0.4713 - acc: 0.7744 - val_loss: 0.7105 - val_acc: 0.5804\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 807us/step - loss: 0.4685 - acc: 0.7873 - val_loss: 0.6820 - val_acc: 0.6161\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 805us/step - loss: 0.4853 - acc: 0.7555 - val_loss: 0.6811 - val_acc: 0.6071\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.4725 - acc: 0.7565 - val_loss: 0.6800 - val_acc: 0.6250\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4723 - acc: 0.7594 - val_loss: 0.6670 - val_acc: 0.6607\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.4815 - acc: 0.7624 - val_loss: 0.6954 - val_acc: 0.6161\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.4800 - acc: 0.7684 - val_loss: 0.6826 - val_acc: 0.6339\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 720us/step - loss: 0.4565 - acc: 0.7704 - val_loss: 0.7000 - val_acc: 0.5982\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4779 - acc: 0.7594 - val_loss: 0.6865 - val_acc: 0.5982\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4829 - acc: 0.7624 - val_loss: 0.6753 - val_acc: 0.6339\n"
     ]
    }
   ],
   "source": [
    "nn_x5 = multi_x5.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y5 =  multi_y5.values\n",
    "nn_y5_train = to_categorical(nn_y5)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x5,nn_y5_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.75      0.85      2867\n",
      "    class 1       0.10      0.62      0.18       133\n",
      "\n",
      "avg / total       0.94      0.74      0.82      3000\n",
      "\n",
      "0.7413333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions5_3 = model.predict(nn_val)\n",
    "pred5_3 = []\n",
    "for i in range(3000):\n",
    "    pred5_3.append(np.argmax(predictions5_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred5_3, target_names=target_names))\n",
    "\n",
    "val5_3_acc = metrics.accuracy_score(y_val,pred5_3)#验证集上的auc值\n",
    "print(val5_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.82      2867\n",
      "    class 1       0.10      0.68      0.17       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.7006666666666667\n"
     ]
    }
   ],
   "source": [
    "predictions5 = predictions5_1+predictions5_2+pred5_3\n",
    "for i in range(3000):\n",
    "    if predictions5[i] >= 2:\n",
    "        predictions5[i] = 1\n",
    "    else:\n",
    "        predictions5[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions5, target_names=target_names))\n",
    "\n",
    "val5_acc = metrics.accuracy_score(y_val,predictions5)#验证集上的auc值\n",
    "print(val5_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6458  1701  2888 13226  7362 11230 10906  8327  3959  7883  1215  4555\n",
      " 13223  7452 13473  9219 10437  7483 10140  8012 12301  9480 13247 14352\n",
      "  4376  4549  1135  6735  6798  6818 14369  3194  2742 14335  8397  2383\n",
      "  6709 10968  2413 13962  3217  5345  6337  1461  3349  3420 12226 10179\n",
      "  1631  1759  6972 13086  2006 13812    10 14096 14610  7221  1847 13068\n",
      "  7574  1267  8580  9377  2819  7945 10966  3836 12888  3590  7657  4537\n",
      " 13802 13569  8052  4169 14041   769 10838 13480  7731  6630  4070  2672\n",
      "  5862  7008 14171  5337 14355 11175  2676  5753  6429 14629  7750  5655\n",
      "  6913  4237  3906  3645 10953  7581  2260   690  3545 13149 12538 10947\n",
      "  6225 13888 13845 11443 14465  6756 12680  4302  2454 12281  3256 13390\n",
      "  4395 13249  4064  5786 11838 10834 10578 12322  4794  8320 13971  3962\n",
      " 13487 14553  9511  1984  9275  3469  8617 14293 10466   293 12887 13646\n",
      " 13846  7254  1068 13088    43 13230 10940 10432 10913 12589 10324 14643\n",
      "   236  2068 13772  5708 10399  7683 13149  7678 11811   326 13749  5164\n",
      " 10900  2579  2935 14476  9030 13871  6865  9486 10485  6006 14637  9174\n",
      " 14665  2294 10387 14596  2455  8380  2758  2473   699   461  7439  5373\n",
      "  7895  7298  3988 10321  6671  4297  2649  4302  1369  8780 12157  8482\n",
      "  9524  8982  2435  5547 11522  7052  2479 14330  4035 11967  8336  5527\n",
      "  8121  3200  8973  2205 13609  1213   550 14303  2170 10115 10383  8277\n",
      "  3534  2275  8563  2905  6123  8373   202  3366  1872  3872 10759  5086\n",
      "  2045 12122  3330 10970  2940  6815 10594  3612  3715 13871  8235  1386\n",
      "  8848  3443  6917   310  5889  1356  8491 11184  2448   806   711 12634\n",
      "  6079  8931 10976  4698  9360 10610  3426 14452 12888  9004  4036 12610\n",
      " 13866  7002 10593 14742  2144  8753 11382  7239   534 13345  8732  8028\n",
      " 11112  2377 13381 14793 14243  6826  3853 11006 14164 12652  7899 10506\n",
      " 13477  8375 14313  7306   714  1902 14153  1693 14615  8855  9063 11675\n",
      "  5890 13814  9066    69  5773   528  7513  1541  8870  7151 12714  8776\n",
      "  8193  5299  2072  2276  1962  7026  4600 12904 11653  1623  7149   433\n",
      "  8535  4316   877 12686  6539  6542  4584 12841  6441  3716   388 14417\n",
      "  2170 13959   998 10226  1856 13763  3555  5283  3297  4519 14635   198\n",
      "  6472 11573  4338  3851 12492   880  5517 12107  7511 13986 12430  8553\n",
      " 14774 10190  4117   591 10454 10273  7365  6336  4744  7890  2006 12612\n",
      "  5568  1302 12904 12527 13070  3102 13655  9098  6754  3074 10347 10538\n",
      " 10259  4284 11601  4523  7001 10908  2344  7398  2650 14507 12978  8467\n",
      "  7966  7450 11996  4723  8539  8674  5905  2227 12388   574 13873 11604\n",
      "  2230  6548 13431  2496 13597  6491  1080  6004  5518   578  1175 11987\n",
      " 12758 11072 14855  9011  3389  5735  6490 11605  2720 14432  3698  3768\n",
      "  6752  2293  4101  8262  3017 10979 13419   584  5823   349 10910 12292\n",
      "   677  4074  1705  4529  3317 12107 14129 10930 14737 14728  1797 11560\n",
      "  2856   478 14518  5565  7180  1981  3356   615  7652  4522 13319  4816\n",
      " 10888 11523  7330  5857 14177  3812  1614 11970   567  2584  1852 13972\n",
      "   308  3895 14034  6523 12394  1754 12636  3242  8924  5125   381  5847\n",
      "  2742   900 13035  8834 14657  7309  5577  9244  6935  3878 10513  6658\n",
      " 11569  1140  3009  7009 10151  5195 11313  4803  6696 12975  3319 13171\n",
      " 12769   467  8311  7330 11559    85 14445  8390 12267 14334  3356 14262\n",
      " 14144  3893 12826   409  3569 10395 10379 14695 12977 12563  3751  4338\n",
      " 10732  2801  2256  5462 12983  3254  2037  1171]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train6 = resample_data(train_x1, 560)\n",
    "print(train6.shape)\n",
    "multi_x6, multi_y6 = concat_data(train6, train_x2)\n",
    "print(multi_x6.shape)\n",
    "print(multi_y6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.67      0.80      2867\n",
      "    class 1       0.10      0.75      0.17       133\n",
      "\n",
      "avg / total       0.94      0.68      0.77      3000\n",
      "\n",
      "0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm6_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x6, multi_y6)\n",
    "predictions6_1 = gbm6_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions6_1, target_names=target_names))\n",
    "\n",
    "val6_1_acc = metrics.accuracy_score(y_val,predictions6_1)#验证集上的auc值\n",
    "print(val6_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.67      0.80      2867\n",
      "    class 1       0.09      0.74      0.17       133\n",
      "\n",
      "avg / total       0.94      0.68      0.77      3000\n",
      "\n",
      "0.677\n"
     ]
    }
   ],
   "source": [
    "clf6_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x6, multi_y6)\n",
    "predictions6_2 = clf6_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions6_2, target_names=target_names))\n",
    "\n",
    "val6_2_acc = metrics.accuracy_score(y_val,predictions6_2)#验证集上的auc值\n",
    "print(val6_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 5s 5ms/step - loss: 0.6810 - acc: 0.5557 - val_loss: 2.0807 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.6496 - acc: 0.6083 - val_loss: 2.2064 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.5958 - acc: 0.6789 - val_loss: 2.6109 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 769us/step - loss: 0.5828 - acc: 0.6899 - val_loss: 2.2600 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.5726 - acc: 0.6859 - val_loss: 1.2636 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 802us/step - loss: 0.5750 - acc: 0.6998 - val_loss: 1.4467 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 784us/step - loss: 0.5633 - acc: 0.7147 - val_loss: 1.5006 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 833us/step - loss: 0.5603 - acc: 0.7068 - val_loss: 1.7145 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 840us/step - loss: 0.5404 - acc: 0.7286 - val_loss: 1.5191 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 752us/step - loss: 0.5403 - acc: 0.7296 - val_loss: 1.2317 - val_acc: 0.0625\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.5468 - acc: 0.7336 - val_loss: 0.1785 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 781us/step - loss: 0.5557 - acc: 0.7038 - val_loss: 1.1454 - val_acc: 0.0804\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.5325 - acc: 0.7356 - val_loss: 0.9077 - val_acc: 0.3839\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 833us/step - loss: 0.5466 - acc: 0.7276 - val_loss: 0.8386 - val_acc: 0.3750\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 801us/step - loss: 0.5331 - acc: 0.7316 - val_loss: 0.7139 - val_acc: 0.5536\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 759us/step - loss: 0.5344 - acc: 0.7465 - val_loss: 0.6379 - val_acc: 0.6607\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 756us/step - loss: 0.5261 - acc: 0.7425 - val_loss: 0.5930 - val_acc: 0.7143\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 776us/step - loss: 0.5301 - acc: 0.7455 - val_loss: 0.6350 - val_acc: 0.6607\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 762us/step - loss: 0.5255 - acc: 0.7396 - val_loss: 0.6772 - val_acc: 0.6071\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 769us/step - loss: 0.5148 - acc: 0.7406 - val_loss: 0.4807 - val_acc: 0.7857\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.5239 - acc: 0.7475 - val_loss: 0.6022 - val_acc: 0.7054\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.5236 - acc: 0.7435 - val_loss: 0.6562 - val_acc: 0.6339\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 771us/step - loss: 0.5182 - acc: 0.7495 - val_loss: 0.5450 - val_acc: 0.7321\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 772us/step - loss: 0.5167 - acc: 0.7445 - val_loss: 0.4755 - val_acc: 0.8036\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 765us/step - loss: 0.5170 - acc: 0.7416 - val_loss: 0.5382 - val_acc: 0.7679\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 767us/step - loss: 0.5248 - acc: 0.7416 - val_loss: 0.7608 - val_acc: 0.4911\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 800us/step - loss: 0.5247 - acc: 0.7346 - val_loss: 0.5453 - val_acc: 0.7589\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 771us/step - loss: 0.5028 - acc: 0.7584 - val_loss: 0.5859 - val_acc: 0.6964\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 801us/step - loss: 0.5034 - acc: 0.7575 - val_loss: 0.5743 - val_acc: 0.7321\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 769us/step - loss: 0.4890 - acc: 0.7694 - val_loss: 0.5429 - val_acc: 0.7500\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 781us/step - loss: 0.4943 - acc: 0.7455 - val_loss: 0.4868 - val_acc: 0.7946\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 832us/step - loss: 0.5096 - acc: 0.7495 - val_loss: 0.5573 - val_acc: 0.7143\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 775us/step - loss: 0.5079 - acc: 0.7575 - val_loss: 0.5417 - val_acc: 0.7321\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.5086 - acc: 0.7515 - val_loss: 0.5790 - val_acc: 0.7054\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 784us/step - loss: 0.5082 - acc: 0.7545 - val_loss: 0.6452 - val_acc: 0.6429\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 784us/step - loss: 0.4973 - acc: 0.7584 - val_loss: 0.5989 - val_acc: 0.6875\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 817us/step - loss: 0.4836 - acc: 0.7684 - val_loss: 0.5837 - val_acc: 0.7143\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 812us/step - loss: 0.4987 - acc: 0.7694 - val_loss: 0.6361 - val_acc: 0.6429\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 777us/step - loss: 0.4869 - acc: 0.7495 - val_loss: 0.6045 - val_acc: 0.7143\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 1s 795us/step - loss: 0.4918 - acc: 0.7654 - val_loss: 0.6199 - val_acc: 0.6786\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 816us/step - loss: 0.5018 - acc: 0.7604 - val_loss: 0.6178 - val_acc: 0.6786\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.4800 - acc: 0.7584 - val_loss: 0.6467 - val_acc: 0.6429\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 767us/step - loss: 0.4900 - acc: 0.7505 - val_loss: 0.6599 - val_acc: 0.6339\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 878us/step - loss: 0.4844 - acc: 0.7714 - val_loss: 0.6071 - val_acc: 0.6786\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 828us/step - loss: 0.4730 - acc: 0.7654 - val_loss: 0.6327 - val_acc: 0.6429\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 774us/step - loss: 0.4766 - acc: 0.7763 - val_loss: 0.6374 - val_acc: 0.6339\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.4536 - acc: 0.7833 - val_loss: 0.6487 - val_acc: 0.6161\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.4932 - acc: 0.7604 - val_loss: 0.6665 - val_acc: 0.6071\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.4876 - acc: 0.7664 - val_loss: 0.6656 - val_acc: 0.5893\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4881 - acc: 0.7644 - val_loss: 0.6510 - val_acc: 0.6339\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4813 - acc: 0.7634 - val_loss: 0.6951 - val_acc: 0.5982\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4575 - acc: 0.7694 - val_loss: 0.6209 - val_acc: 0.6518\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 734us/step - loss: 0.4815 - acc: 0.7604 - val_loss: 0.6413 - val_acc: 0.6250\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.4571 - acc: 0.7853 - val_loss: 0.6485 - val_acc: 0.6250\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 818us/step - loss: 0.4597 - acc: 0.7694 - val_loss: 0.6533 - val_acc: 0.6250\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 800us/step - loss: 0.4757 - acc: 0.7753 - val_loss: 0.6660 - val_acc: 0.6071\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.4687 - acc: 0.7684 - val_loss: 0.6621 - val_acc: 0.6071\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4672 - acc: 0.7753 - val_loss: 0.6286 - val_acc: 0.6161\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.4781 - acc: 0.7763 - val_loss: 0.6490 - val_acc: 0.6250\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 742us/step - loss: 0.4611 - acc: 0.7634 - val_loss: 0.6398 - val_acc: 0.6429\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 754us/step - loss: 0.4562 - acc: 0.7734 - val_loss: 0.6111 - val_acc: 0.6518\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4588 - acc: 0.7714 - val_loss: 0.6394 - val_acc: 0.6339\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.4638 - acc: 0.7734 - val_loss: 0.6622 - val_acc: 0.6339\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4460 - acc: 0.7704 - val_loss: 0.6334 - val_acc: 0.6339\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4563 - acc: 0.7773 - val_loss: 0.6413 - val_acc: 0.6518\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4616 - acc: 0.7813 - val_loss: 0.6810 - val_acc: 0.6071\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4632 - acc: 0.7793 - val_loss: 0.6146 - val_acc: 0.6786\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.4631 - acc: 0.7734 - val_loss: 0.7009 - val_acc: 0.6071\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4682 - acc: 0.7565 - val_loss: 0.6248 - val_acc: 0.6875\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4619 - acc: 0.7753 - val_loss: 0.6739 - val_acc: 0.6339\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 726us/step - loss: 0.4618 - acc: 0.7545 - val_loss: 0.6223 - val_acc: 0.6964\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4451 - acc: 0.7952 - val_loss: 0.6663 - val_acc: 0.6250\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.4607 - acc: 0.7763 - val_loss: 0.6536 - val_acc: 0.6518\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4585 - acc: 0.7684 - val_loss: 0.6873 - val_acc: 0.6250\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 720us/step - loss: 0.4481 - acc: 0.7952 - val_loss: 0.7103 - val_acc: 0.6161\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.4367 - acc: 0.7853 - val_loss: 0.6879 - val_acc: 0.6339\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 734us/step - loss: 0.4349 - acc: 0.7932 - val_loss: 0.6940 - val_acc: 0.6429\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4527 - acc: 0.7903 - val_loss: 0.6714 - val_acc: 0.6518\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 730us/step - loss: 0.4465 - acc: 0.7763 - val_loss: 0.6970 - val_acc: 0.6429\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4247 - acc: 0.7873 - val_loss: 0.7014 - val_acc: 0.6250\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 718us/step - loss: 0.4495 - acc: 0.7823 - val_loss: 0.6771 - val_acc: 0.6429\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4332 - acc: 0.7972 - val_loss: 0.6876 - val_acc: 0.6518\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4429 - acc: 0.7932 - val_loss: 0.6618 - val_acc: 0.6429\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4409 - acc: 0.8022 - val_loss: 0.6842 - val_acc: 0.6250\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.4051 - acc: 0.8042 - val_loss: 0.7340 - val_acc: 0.6071\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.4357 - acc: 0.7972 - val_loss: 0.6802 - val_acc: 0.6339\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4466 - acc: 0.7744 - val_loss: 0.7154 - val_acc: 0.6339\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4558 - acc: 0.7853 - val_loss: 0.6703 - val_acc: 0.6607\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 721us/step - loss: 0.4357 - acc: 0.7893 - val_loss: 0.6834 - val_acc: 0.6696\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4551 - acc: 0.7883 - val_loss: 0.6683 - val_acc: 0.6429\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.4487 - acc: 0.7853 - val_loss: 0.6798 - val_acc: 0.6161\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4098 - acc: 0.7962 - val_loss: 0.6771 - val_acc: 0.6429\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 733us/step - loss: 0.4344 - acc: 0.7962 - val_loss: 0.6757 - val_acc: 0.6518\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4289 - acc: 0.7903 - val_loss: 0.6645 - val_acc: 0.6696\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.4289 - acc: 0.7942 - val_loss: 0.7265 - val_acc: 0.6250\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4293 - acc: 0.8052 - val_loss: 0.7163 - val_acc: 0.6250\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4307 - acc: 0.8002 - val_loss: 0.7207 - val_acc: 0.6429\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.4422 - acc: 0.7942 - val_loss: 0.7236 - val_acc: 0.6607\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 726us/step - loss: 0.4279 - acc: 0.8002 - val_loss: 0.6928 - val_acc: 0.6518\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 722us/step - loss: 0.4260 - acc: 0.8082 - val_loss: 0.6884 - val_acc: 0.6696\n"
     ]
    }
   ],
   "source": [
    "nn_x6 = multi_x6.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y6 =  multi_y6.values\n",
    "nn_y6_train = to_categorical(nn_y6)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x6,nn_y6_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.69      0.81      2867\n",
      "    class 1       0.09      0.69      0.16       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.689\n"
     ]
    }
   ],
   "source": [
    "predictions6_3 = model.predict(nn_val)\n",
    "pred6_3 = []\n",
    "for i in range(3000):\n",
    "    pred6_3.append(np.argmax(predictions6_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred6_3, target_names=target_names))\n",
    "\n",
    "val6_3_acc = metrics.accuracy_score(y_val,pred6_3)#验证集上的auc值\n",
    "print(val6_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.81      2867\n",
      "    class 1       0.10      0.76      0.18       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.6873333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions6 = predictions6_1+predictions6_2+pred6_3\n",
    "for i in range(3000):\n",
    "    if predictions6[i] >= 2:\n",
    "        predictions6[i] = 1\n",
    "    else:\n",
    "        predictions6[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions6, target_names=target_names))\n",
    "\n",
    "val6_acc = metrics.accuracy_score(y_val,predictions6)#验证集上的auc值\n",
    "print(val6_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  562 14507  1577 10602  1135 13151 14751 13248 10042  1249  3251  1761\n",
      "  2428 10506  2440 12047 13145  6150  7660 11300 12244 10160  2100   569\n",
      "   710  6960 10302 13299  1746 11796  8201  7936  8694  7303 11476 11305\n",
      "  5323 10198 13425  2535  2855 12739  8012 14252  6951  1840  7594  8349\n",
      "  5277 13582 13680  9377  9536 10735 11330  8145  2981   913  8224 11987\n",
      "  1835  6503  3282  1162  8033  5578  9184   423  4444  7855 11035  9304\n",
      "  5980  5494 14337   897  3471  9516 14844  6900  4837   796  8502  5205\n",
      " 14282  7839 12488 13880  1717  6169   401 13257 11189  6859  6113 14416\n",
      " 11075 10421  3335  9308  5972 12933  7335   817   854  5020  1907 12347\n",
      "  1471 14685  5604  6929 13646 14095  7595  8637  3787  1830 11666  1726\n",
      "  8141  7188  8909  5658  3226  4564 10172 10890  7845 10238  1707 12180\n",
      "  3349  6380  6780   690  1691  5848  7008 11883   521 10654  9437 14420\n",
      "  3365  1242 10669 12221  5462  4107  1150 12565 13045  3538 12967  8332\n",
      " 13023  5737  3883   894  8797 10880  8750  1129 14187  2184  1102  6584\n",
      " 13624 11891  2623 14090 11237 11951  4142  4450  1920  7769  9177  3163\n",
      "  4595 13055 13635  1346 10205 13400   548 10020 10376  8831  4717 11291\n",
      "  2992  3752  5349  8008  8961  2722  4796  5458  2346  1614  4271 11534\n",
      "  7124  5129  6514  9321  8450   339  5874  7961   229 14315 12239  4007\n",
      "  8911  4696  9368  8722   471 12118 10771  1942  4137  5166 10992  2193\n",
      "  9472 12280 13152  4462 12311  2358  2580   274  8197  2759  5899  4700\n",
      "  2414  4313  7686  1270  7140  2359  4898 13394 12667  9168  9083 13145\n",
      " 14311  1948  8268 14660 13785 11467  6796  3708   605 13314  2852 14111\n",
      " 10553  1485  5589  9186 13951 10805 11897 14192  6004  6002  8953 12284\n",
      "  3764  2073 13483  8659   514  7449  8456 13777  5799 12290 14702  5718\n",
      "  4001  1831 11916  9004  2839  7657  8833  9312  7122  4803 11321 12035\n",
      "  7307  3688 13294  5754   317  4755  3028   740  7451 12109  6157 11487\n",
      "  5513  3764 10217   343  7494  2516  8995   852  6384 12480  5963 13394\n",
      " 13492  4208 12350 13945  8279 13195 12188  4341  6032  2374  8255  5294\n",
      " 13388 12853 13424  9151  2156  1524  1753  7064  9245  7304  2360 12509\n",
      "  9206  5055  7598 10826 13664  1343  2503 11980  8595 10611 13151  4184\n",
      "  3381  8595 14422 13945  7723  8556  7963  4310  6817  6128  4697 11486\n",
      "  4651  4110   236  5514  9384 11355  1999 10658 10886  4027 12616  7940\n",
      "  1200  6226  3344 13872  6734 14818  5353 13044 10659  5170 11697 12054\n",
      "  4185  3899 12466  8573  1768  5332  6960  1648 10535  9002  3735  8772\n",
      "   896 11342  8328 12465 12804  4037   815  9133  6414  8164 13915  5053\n",
      "  9330  7583  7625  2309 13920  4750   618  2211 11909  8732  4854  7730\n",
      "  2638 14211 13828  4862  4621  6915 14130  1009  9253  3228  9308  8142\n",
      "  4144  8650  2243  7390 12504  1928  7118 14579  7065  5208  3026  2173\n",
      "  2478 12847  9247  8695 14652  6589 13338 14560 13258 13993  2229  7199\n",
      "  4291 11473  7319  9482  1859 13808 11721  8294 12599  7962 12758   554\n",
      " 11217 10460  3979 13048  2510  7774  1445  7536 11493 12853  1870  1266\n",
      "  3232  4844 12924  8447 12823  4270  9346  1506  6961  7388 11536  2434\n",
      "  7858  3630  3652  6077 11789  6493  9066  9355 13286  5210   555  2448\n",
      "  2213 12938  5836   869   462 13429 14313  9523  3409  1954  9289  9221\n",
      " 13026    87 12391  5305 14179  5545 12051  9091  6776 13523 14742 14145\n",
      "  2986 12036  4658   472 13198 10137 14479  2640 10457  2493   273  4674\n",
      "  7917   546  9272  1650  4860 11687 13012  1595]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train7 = resample_data(train_x1, 560)\n",
    "print(train7.shape)\n",
    "multi_x7, multi_y7 = concat_data(train7, train_x2)\n",
    "print(multi_x7.shape)\n",
    "print(multi_y7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.71      0.82      2867\n",
      "    class 1       0.10      0.72      0.18       133\n",
      "\n",
      "avg / total       0.94      0.71      0.79      3000\n",
      "\n",
      "0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm7_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x7, multi_y7)\n",
    "predictions7_1 = gbm7_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions7_1, target_names=target_names))\n",
    "\n",
    "val7_1_acc = metrics.accuracy_score(y_val,predictions7_1)#验证集上的auc值\n",
    "print(val7_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.69      0.81      2867\n",
      "    class 1       0.10      0.74      0.17       133\n",
      "\n",
      "avg / total       0.94      0.69      0.78      3000\n",
      "\n",
      "0.691\n"
     ]
    }
   ],
   "source": [
    "clf7_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,max_depth=6, random_state=0).fit(multi_x7, multi_y7)\n",
    "predictions7_2 = clf7_2.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions7_2, target_names=target_names))\n",
    "\n",
    "val7_2_acc = metrics.accuracy_score(y_val,predictions7_2)#验证集上的auc值\n",
    "print(val7_2_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1006 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1006/1006 [==============================] - 4s 4ms/step - loss: 0.6874 - acc: 0.5616 - val_loss: 1.1592 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1006/1006 [==============================] - 1s 741us/step - loss: 0.6514 - acc: 0.6173 - val_loss: 0.1723 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.6097 - acc: 0.6610 - val_loss: 4.5224e-04 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "1006/1006 [==============================] - 1s 741us/step - loss: 0.6092 - acc: 0.6720 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "1006/1006 [==============================] - 1s 757us/step - loss: 0.6022 - acc: 0.6690 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "1006/1006 [==============================] - 1s 791us/step - loss: 0.5760 - acc: 0.7097 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "1006/1006 [==============================] - 1s 754us/step - loss: 0.5868 - acc: 0.6859 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "1006/1006 [==============================] - 1s 774us/step - loss: 0.5644 - acc: 0.6968 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5577 - acc: 0.6978 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.5646 - acc: 0.7167 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1006/1006 [==============================] - 1s 764us/step - loss: 0.5598 - acc: 0.6859 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.5554 - acc: 0.7097 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.5469 - acc: 0.7197 - val_loss: 0.0240 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1006/1006 [==============================] - 1s 759us/step - loss: 0.5616 - acc: 0.7028 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "1006/1006 [==============================] - 1s 807us/step - loss: 0.5346 - acc: 0.7286 - val_loss: 0.0206 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.5466 - acc: 0.7197 - val_loss: 0.0960 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1006/1006 [==============================] - 1s 762us/step - loss: 0.5591 - acc: 0.7107 - val_loss: 0.1752 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "1006/1006 [==============================] - 1s 725us/step - loss: 0.5364 - acc: 0.7296 - val_loss: 0.1951 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "1006/1006 [==============================] - 1s 744us/step - loss: 0.5433 - acc: 0.7167 - val_loss: 0.2262 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.5397 - acc: 0.7207 - val_loss: 0.2505 - val_acc: 0.9821\n",
      "Epoch 21/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.5402 - acc: 0.7256 - val_loss: 0.1669 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "1006/1006 [==============================] - 1s 733us/step - loss: 0.5545 - acc: 0.7217 - val_loss: 0.1762 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.5340 - acc: 0.7336 - val_loss: 0.1949 - val_acc: 0.9821\n",
      "Epoch 24/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.5160 - acc: 0.7366 - val_loss: 0.2191 - val_acc: 0.9821\n",
      "Epoch 25/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.5330 - acc: 0.7177 - val_loss: 0.3359 - val_acc: 0.8929\n",
      "Epoch 26/100\n",
      "1006/1006 [==============================] - 1s 737us/step - loss: 0.5461 - acc: 0.7187 - val_loss: 0.3842 - val_acc: 0.8571\n",
      "Epoch 27/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.5361 - acc: 0.7376 - val_loss: 0.4225 - val_acc: 0.8393\n",
      "Epoch 28/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.5120 - acc: 0.7435 - val_loss: 0.4572 - val_acc: 0.8036\n",
      "Epoch 29/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.5296 - acc: 0.7107 - val_loss: 0.4439 - val_acc: 0.8214\n",
      "Epoch 30/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.5303 - acc: 0.7366 - val_loss: 0.4805 - val_acc: 0.7946\n",
      "Epoch 31/100\n",
      "1006/1006 [==============================] - 1s 723us/step - loss: 0.5344 - acc: 0.7247 - val_loss: 0.4276 - val_acc: 0.8036\n",
      "Epoch 32/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.5144 - acc: 0.7545 - val_loss: 0.4868 - val_acc: 0.7321\n",
      "Epoch 33/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.5194 - acc: 0.7227 - val_loss: 0.4672 - val_acc: 0.7768\n",
      "Epoch 34/100\n",
      "1006/1006 [==============================] - 1s 757us/step - loss: 0.5147 - acc: 0.7435 - val_loss: 0.4843 - val_acc: 0.7500\n",
      "Epoch 35/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.5060 - acc: 0.7306 - val_loss: 0.5710 - val_acc: 0.6875\n",
      "Epoch 36/100\n",
      "1006/1006 [==============================] - 1s 821us/step - loss: 0.5192 - acc: 0.7406 - val_loss: 0.5105 - val_acc: 0.7232\n",
      "Epoch 37/100\n",
      "1006/1006 [==============================] - 1s 768us/step - loss: 0.5114 - acc: 0.7386 - val_loss: 0.5504 - val_acc: 0.7054\n",
      "Epoch 38/100\n",
      "1006/1006 [==============================] - 1s 724us/step - loss: 0.5180 - acc: 0.7416 - val_loss: 0.5797 - val_acc: 0.6875\n",
      "Epoch 39/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.5032 - acc: 0.7515 - val_loss: 0.5663 - val_acc: 0.6875\n",
      "Epoch 40/100\n",
      "1006/1006 [==============================] - 1s 752us/step - loss: 0.5031 - acc: 0.7525 - val_loss: 0.6112 - val_acc: 0.6518\n",
      "Epoch 41/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.5034 - acc: 0.7535 - val_loss: 0.6517 - val_acc: 0.5982\n",
      "Epoch 42/100\n",
      "1006/1006 [==============================] - 1s 733us/step - loss: 0.5017 - acc: 0.7584 - val_loss: 0.5892 - val_acc: 0.6607\n",
      "Epoch 43/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.5039 - acc: 0.7535 - val_loss: 0.5912 - val_acc: 0.6518\n",
      "Epoch 44/100\n",
      "1006/1006 [==============================] - 1s 734us/step - loss: 0.4963 - acc: 0.7505 - val_loss: 0.6238 - val_acc: 0.6339\n",
      "Epoch 45/100\n",
      "1006/1006 [==============================] - 1s 758us/step - loss: 0.5216 - acc: 0.7386 - val_loss: 0.6089 - val_acc: 0.6786\n",
      "Epoch 46/100\n",
      "1006/1006 [==============================] - 1s 750us/step - loss: 0.5099 - acc: 0.7515 - val_loss: 0.6262 - val_acc: 0.6607\n",
      "Epoch 47/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.5088 - acc: 0.7445 - val_loss: 0.6604 - val_acc: 0.5982\n",
      "Epoch 48/100\n",
      "1006/1006 [==============================] - 1s 740us/step - loss: 0.5114 - acc: 0.7515 - val_loss: 0.6178 - val_acc: 0.6607\n",
      "Epoch 49/100\n",
      "1006/1006 [==============================] - 1s 774us/step - loss: 0.5084 - acc: 0.7535 - val_loss: 0.6333 - val_acc: 0.6250\n",
      "Epoch 50/100\n",
      "1006/1006 [==============================] - 1s 789us/step - loss: 0.4996 - acc: 0.7505 - val_loss: 0.6448 - val_acc: 0.5893\n",
      "Epoch 51/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.4898 - acc: 0.7505 - val_loss: 0.6904 - val_acc: 0.5714\n",
      "Epoch 52/100\n",
      "1006/1006 [==============================] - 1s 750us/step - loss: 0.5056 - acc: 0.7435 - val_loss: 0.6441 - val_acc: 0.5982\n",
      "Epoch 53/100\n",
      "1006/1006 [==============================] - 1s 769us/step - loss: 0.4738 - acc: 0.7654 - val_loss: 0.6796 - val_acc: 0.5804\n",
      "Epoch 54/100\n",
      "1006/1006 [==============================] - 1s 739us/step - loss: 0.4929 - acc: 0.7356 - val_loss: 0.7201 - val_acc: 0.5446\n",
      "Epoch 55/100\n",
      "1006/1006 [==============================] - 1s 738us/step - loss: 0.4999 - acc: 0.7485 - val_loss: 0.6647 - val_acc: 0.5893\n",
      "Epoch 56/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.4857 - acc: 0.7584 - val_loss: 0.7036 - val_acc: 0.5536\n",
      "Epoch 57/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.4833 - acc: 0.7624 - val_loss: 0.6848 - val_acc: 0.5893\n",
      "Epoch 58/100\n",
      "1006/1006 [==============================] - 1s 728us/step - loss: 0.4880 - acc: 0.7555 - val_loss: 0.7346 - val_acc: 0.5268\n",
      "Epoch 59/100\n",
      "1006/1006 [==============================] - 1s 726us/step - loss: 0.4910 - acc: 0.7515 - val_loss: 0.6826 - val_acc: 0.5893\n",
      "Epoch 60/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4981 - acc: 0.7416 - val_loss: 0.6904 - val_acc: 0.5714\n",
      "Epoch 61/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.4744 - acc: 0.7724 - val_loss: 0.6356 - val_acc: 0.6339\n",
      "Epoch 62/100\n",
      "1006/1006 [==============================] - 1s 791us/step - loss: 0.4957 - acc: 0.7614 - val_loss: 0.7175 - val_acc: 0.5357\n",
      "Epoch 63/100\n",
      "1006/1006 [==============================] - 1s 768us/step - loss: 0.4983 - acc: 0.7465 - val_loss: 0.7341 - val_acc: 0.5357\n",
      "Epoch 64/100\n",
      "1006/1006 [==============================] - 1s 773us/step - loss: 0.4841 - acc: 0.7634 - val_loss: 0.6372 - val_acc: 0.6339\n",
      "Epoch 65/100\n",
      "1006/1006 [==============================] - 1s 840us/step - loss: 0.4903 - acc: 0.7535 - val_loss: 0.7386 - val_acc: 0.5268\n",
      "Epoch 66/100\n",
      "1006/1006 [==============================] - 1s 816us/step - loss: 0.4649 - acc: 0.7634 - val_loss: 0.6968 - val_acc: 0.5982\n",
      "Epoch 67/100\n",
      "1006/1006 [==============================] - 1s 834us/step - loss: 0.4919 - acc: 0.7704 - val_loss: 0.6967 - val_acc: 0.5804\n",
      "Epoch 68/100\n",
      "1006/1006 [==============================] - 1s 782us/step - loss: 0.4884 - acc: 0.7545 - val_loss: 0.7149 - val_acc: 0.5357\n",
      "Epoch 69/100\n",
      "1006/1006 [==============================] - 1s 785us/step - loss: 0.4963 - acc: 0.7475 - val_loss: 0.7143 - val_acc: 0.5536\n",
      "Epoch 70/100\n",
      "1006/1006 [==============================] - 1s 773us/step - loss: 0.4744 - acc: 0.7475 - val_loss: 0.7159 - val_acc: 0.5893\n",
      "Epoch 71/100\n",
      "1006/1006 [==============================] - 1s 770us/step - loss: 0.4740 - acc: 0.7634 - val_loss: 0.6881 - val_acc: 0.5982\n",
      "Epoch 72/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.4873 - acc: 0.7714 - val_loss: 0.7512 - val_acc: 0.5357\n",
      "Epoch 73/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4755 - acc: 0.7674 - val_loss: 0.6906 - val_acc: 0.6161\n",
      "Epoch 74/100\n",
      "1006/1006 [==============================] - 1s 761us/step - loss: 0.4680 - acc: 0.7644 - val_loss: 0.7558 - val_acc: 0.5714\n",
      "Epoch 75/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4552 - acc: 0.7843 - val_loss: 0.6922 - val_acc: 0.6250\n",
      "Epoch 76/100\n",
      "1006/1006 [==============================] - 1s 729us/step - loss: 0.4743 - acc: 0.7545 - val_loss: 0.7409 - val_acc: 0.5625\n",
      "Epoch 77/100\n",
      "1006/1006 [==============================] - 1s 727us/step - loss: 0.4736 - acc: 0.7485 - val_loss: 0.7142 - val_acc: 0.5982\n",
      "Epoch 78/100\n",
      "1006/1006 [==============================] - 1s 732us/step - loss: 0.4664 - acc: 0.7594 - val_loss: 0.6804 - val_acc: 0.6250\n",
      "Epoch 79/100\n",
      "1006/1006 [==============================] - 1s 735us/step - loss: 0.4791 - acc: 0.7654 - val_loss: 0.7761 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "1006/1006 [==============================] - 1s 731us/step - loss: 0.4803 - acc: 0.7644 - val_loss: 0.7064 - val_acc: 0.5714\n",
      "Epoch 81/100\n",
      "1006/1006 [==============================] - 1s 736us/step - loss: 0.4712 - acc: 0.7594 - val_loss: 0.7250 - val_acc: 0.5804\n",
      "Epoch 82/100\n",
      "1006/1006 [==============================] - 1s 771us/step - loss: 0.4471 - acc: 0.7863 - val_loss: 0.7456 - val_acc: 0.5804\n",
      "Epoch 83/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.4751 - acc: 0.7833 - val_loss: 0.7662 - val_acc: 0.5714\n",
      "Epoch 84/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.4565 - acc: 0.7942 - val_loss: 0.7315 - val_acc: 0.6161\n",
      "Epoch 85/100\n",
      "1006/1006 [==============================] - 1s 748us/step - loss: 0.4621 - acc: 0.7674 - val_loss: 0.7750 - val_acc: 0.5536\n",
      "Epoch 86/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.4834 - acc: 0.7594 - val_loss: 0.7020 - val_acc: 0.6161\n",
      "Epoch 87/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.4511 - acc: 0.7744 - val_loss: 0.7404 - val_acc: 0.5982\n",
      "Epoch 88/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.4664 - acc: 0.7734 - val_loss: 0.7406 - val_acc: 0.6161\n",
      "Epoch 89/100\n",
      "1006/1006 [==============================] - 1s 743us/step - loss: 0.4527 - acc: 0.7704 - val_loss: 0.7548 - val_acc: 0.5893\n",
      "Epoch 90/100\n",
      "1006/1006 [==============================] - 1s 749us/step - loss: 0.4651 - acc: 0.7724 - val_loss: 0.7558 - val_acc: 0.5982\n",
      "Epoch 91/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.4633 - acc: 0.7654 - val_loss: 0.7630 - val_acc: 0.5804\n",
      "Epoch 92/100\n",
      "1006/1006 [==============================] - 1s 775us/step - loss: 0.4449 - acc: 0.7823 - val_loss: 0.7557 - val_acc: 0.6339\n",
      "Epoch 93/100\n",
      "1006/1006 [==============================] - 1s 752us/step - loss: 0.4421 - acc: 0.7972 - val_loss: 0.7447 - val_acc: 0.6161\n",
      "Epoch 94/100\n",
      "1006/1006 [==============================] - 1s 746us/step - loss: 0.4570 - acc: 0.7823 - val_loss: 0.7458 - val_acc: 0.5982\n",
      "Epoch 95/100\n",
      "1006/1006 [==============================] - 1s 745us/step - loss: 0.4530 - acc: 0.7704 - val_loss: 0.7351 - val_acc: 0.6161\n",
      "Epoch 96/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.4509 - acc: 0.7753 - val_loss: 0.8110 - val_acc: 0.5446\n",
      "Epoch 97/100\n",
      "1006/1006 [==============================] - 1s 750us/step - loss: 0.4637 - acc: 0.7763 - val_loss: 0.7651 - val_acc: 0.5625\n",
      "Epoch 98/100\n",
      "1006/1006 [==============================] - 1s 751us/step - loss: 0.4456 - acc: 0.7773 - val_loss: 0.7578 - val_acc: 0.5893\n",
      "Epoch 99/100\n",
      "1006/1006 [==============================] - 1s 755us/step - loss: 0.4453 - acc: 0.7922 - val_loss: 0.7431 - val_acc: 0.6161\n",
      "Epoch 100/100\n",
      "1006/1006 [==============================] - 1s 747us/step - loss: 0.4475 - acc: 0.7793 - val_loss: 0.7720 - val_acc: 0.5893\n"
     ]
    }
   ],
   "source": [
    "nn_x7 = multi_x7.values\n",
    "nn_val = X_val.values\n",
    "\n",
    "nn_y7 =  multi_y7.values\n",
    "nn_y7_train = to_categorical(nn_y7)\n",
    "\n",
    "nn_y_val =  y_val.values\n",
    "nn_yy_val = to_categorical(nn_y_val)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(361,)))\n",
    "model.add(Reshape((361,1,1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 16, kernel_size = 5,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.65))\n",
    "# model.add(Dense(80, activation = 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(nn_x7,nn_y7_train, batch_size=64, epochs=100, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.78      0.87      2867\n",
      "    class 1       0.11      0.60      0.19       133\n",
      "\n",
      "avg / total       0.94      0.77      0.84      3000\n",
      "\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "predictions7_3 = model.predict(nn_val)\n",
    "pred7_3 = []\n",
    "for i in range(3000):\n",
    "    pred7_3.append(np.argmax(predictions7_3[i]))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, pred7_3, target_names=target_names))\n",
    "\n",
    "val7_3_acc = metrics.accuracy_score(y_val,pred7_3)#验证集上的auc值\n",
    "print(val7_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.72      0.83      2867\n",
      "    class 1       0.11      0.71      0.18       133\n",
      "\n",
      "avg / total       0.94      0.72      0.80      3000\n",
      "\n",
      "0.7213333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions7 = predictions7_1+predictions7_2+pred7_3\n",
    "for i in range(3000):\n",
    "    if predictions7[i] >= 2:\n",
    "        predictions7[i] = 1\n",
    "    else:\n",
    "        predictions7[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions7, target_names=target_names))\n",
    "\n",
    "val7_acc = metrics.accuracy_score(y_val,predictions7)#验证集上的auc值\n",
    "print(val7_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第八个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2060  8570  3310 12257  5423 11895  9017  7890 10110 12406 10135  8861\n",
      "  7123  6640  1998  1372  1025 11983 10751  5647 11012  1307  6965  3717\n",
      "  8410  9211  7131 14149 13066 14623 14610  2364 11193 12394  4448   423\n",
      " 10354 12827  4861   363 10916 13411 12735  2924   393  4115   388  5558\n",
      "  1613   107   814 12457  5968  2326  4206 10424  3131 12950  4144  8414\n",
      "  8039 13138 12199  8999 12068 14090  1953  6999  7965  7879  7545 11882\n",
      "  7656 13523  4069 11604  2536  9110  8050  3954 13835 10904 13955 11537\n",
      "  1845 13494 12372 14672  7409  1052   479 14322  7316  7714  6779  1133\n",
      "  1340 12028  7558  5939  8733 12955  8162  1980  5513  9472 14755  6548\n",
      "   887  8359  2781  5353  5735  2629  4101 12620  3205  4447  7258 12763\n",
      "  9493  6002 13345 13029  7635  8308  6268  2904  8775 13012   294 11082\n",
      "  4677 12452 12571  4252  7066   434 13648 14492  4757  8459  7506  3886\n",
      "  9192 14815 10123  2905  1665  5287  7175 13755 14234 13349 12507  7931\n",
      "  8378  4037 12040 14119  5829 12453  8333  7337 14095  6187  2388 14605\n",
      "  5413  3381 13858  2067 13161 13483  8346  7347 10701  5909 11638 12846\n",
      "  3297 12963 14540 13903 14664  4665  3216  3881 12636  3464 11949   323\n",
      " 13761   798   178 14651  5681  8242   944  1085  5936  5778 12190 12390\n",
      "  7721 13601   105  5858 11961  6748 13116  7273  2986  4443 11276  7841\n",
      " 14170 11279 11046 14171 10853  2034 11022  6466 13017  6034 11570 13970\n",
      "  7352  3435  4106  4229 13659  3284 12180 12858 10627  9144  2254 13714\n",
      " 12991  5246  8320 12902  8180   653  3933 13621 11310  6744 11595 13090\n",
      " 14255   583 11549   632  7852 12804  2857 13684  3008 12772 10554 13564\n",
      " 10571  6854 10996 14673  4726  2653  3307  9403 13826  1772 12197  5617\n",
      "  5915  1204 12655 12040  2410  1584  5970 12849  1229  1668   843  9384\n",
      "     8  3374  4567  1507 10855 11793 13334  5409  5945 13978 12557  7120\n",
      "  5294 10400  1560 12950   804  4503  7688 10518 10357 11451  8000  4337\n",
      " 12292 10074  7131  4303  1620  9264  5388  5017  1921  3761 10041  3531\n",
      "  6579  5644 11924  3872  8909  5368  7295  9468  7808  5497 11383 13395\n",
      "  6704 14107  1280  7599 11954 14446  3664 12925     1 10414  3555 12140\n",
      " 11173  9080 13767   888  4894  1124  2170 13909 12895  7022 12387  6010\n",
      "  3432 11744  3697 11273  4218  7432 14846 11538  2526 13364  3381  4780\n",
      "  1318  3042   608  6094   245 11105  5542  3283  6093 10062   693  8985\n",
      "  2495  6893  5601  2743  8491  9060  6407  2748 12636  1521 11613  2785\n",
      "  2668 13190 13466 12322  2857  3325  4401  2949  1192  4870  2921 14500\n",
      " 11970  7331  4498 13051 11254 14844 10496 13631 11492  7199  8618 10646\n",
      " 12212  5079  3837  3629  6614 10389 10604   697  4017  3352  6954 10589\n",
      "  1787  2864  9125  5957  3751 13414   464 14855  3671 12802 10378  8398\n",
      "  1648  2067  4780  4341  6586  4115  3815  9105  6007  7925 10887  1903\n",
      " 12666 11858  1065  4904 13486  3623  8920  1118  8333 11453 10218   972\n",
      "  3164 13921  7450 14151 12067 13762  8309 12091  8421  3044  3148  5031\n",
      " 13466  3742 11713  5819  9106 14601  6226  9215 11129  1731  4875  2448\n",
      " 13329  9345 14152  7914  4206  7161  7752   285  1595  4267 10246 14288\n",
      "  2471   705 12694  8656  2625  9432  3139  3377  7161  6945  8567 14317\n",
      " 13914 13046 12577 12810  8407 10704 12396 14853  7412 12515 10262 11889\n",
      "  7273 11828  4003 10758  7944 14028 14349  5561 10621  4103 14565  3487\n",
      "  6241 11277  5084  2621  3393  4085  5736    18  4198  5366    78   494\n",
      " 10737  3350  8094  7493   139   482 10735 11803]\n",
      "(560, 362)\n",
      "(1118, 361)\n",
      "(1118,)\n"
     ]
    }
   ],
   "source": [
    "train8 = resample_data(train_x1, 560)\n",
    "print(train8.shape)\n",
    "multi_x8, multi_y8 = concat_data(train8, train_x2)\n",
    "print(multi_x8.shape)\n",
    "print(multi_y8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.68      0.81      2867\n",
      "    class 1       0.10      0.74      0.17       133\n",
      "\n",
      "avg / total       0.94      0.68      0.78      3000\n",
      "\n",
      "0.6846666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm8_1 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x8, multi_y8)\n",
    "predictions8_1 = gbm8_1.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions8_1, target_names=target_names))\n",
    "\n",
    "val8_1_acc = metrics.accuracy_score(y_val,predictions8_1)#验证集上的auc值\n",
    "print(val8_1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14398  8266  1912  2763 13802 12293 14052  2504  1534  7987 12969  4544\n",
      "  8676 10169 14248  8588  9248 12147 14738 14381  4659 14497 11316 12458\n",
      "  1596 14095  4206  6535 11222  3604  9048  5005  4392 11883   941  1052\n",
      "  1670  4093  7151  1025  8027  3224  7420  2526  9465  6592 10376   241\n",
      "  4691 13186 13145   743 14398 14473 12068  8896  5316 10042  5982 12572\n",
      "  8487 12401 13748 14187  8947 12034  5166  6156  5227  4255  8809  2547\n",
      " 12370 12409  4435 10679  1722 12686  3716  5319  2602 10125  2633  4096\n",
      "  2891  6659  7168  8740  1675  9294  8142 14761  5240 11906  2941  8731\n",
      " 11927  6065 13717  7312   905 12780  7067  5005  6700  9110 10520 14004\n",
      "  1626 10553 10590  6454  6733 11643 14721 11548 11664 10577  2718 14480\n",
      " 14420  7770  7464 10436 12262  8106  5920  3339  9485 10591  6136 11237\n",
      "  4108  2004 10312 10195 10428  5059 14438   929  7142 12542  5633   669\n",
      "  5348  5901   588 14152  7984 10038   878 13868   476 13330  6118 12158\n",
      "  6095 14649  2642 12504  7345  4142 10544 14086 11189  7246  3552 10240\n",
      " 11406 10526  4089  5414  6188  7238  4088  3412 14829  8958  8160  1684\n",
      "   924  2345  1265 12365 13099 12285 12475 14670  8569   306 11524  2548\n",
      " 12060  8380   443 10022   700 11311 13838  7644  5288 11448  1852 13147\n",
      "  8926 13530  6740  2599  9412 10592  3480  4749 11281  5650  8091  1063\n",
      "  2153 11138 14390 14243   299 12457 12882 14861 11567  3288  3477  3636\n",
      "  1116  2302 13328  4033  6288 10094 11756 14207  7326 14657  1860  9244\n",
      "  4834  4035  1809   444  6788 12125  1787  5148  8474  4176  2829  3882\n",
      "  2903  8266  5772   842 11833  1685  4856  6010  1808  4675  6855  5590\n",
      "  9426 14004  7634  8925  7629   323  5747  1810  5307  3563  6566 10615\n",
      "  8962 14379  9456  3130  5899  7775  3929  3417  1797  7227 10116  3817\n",
      "  4755 12370  3737  3269 14110  3360  6568 12624  1912  1421  1294   591\n",
      "  1598  1305 10084  6002  7343  6194 10966  3913  2035    85  2830  7560\n",
      "   667 10179  9526  7795   252  7452  5741   179 11536  2540   647 12260\n",
      " 14032  6430 10906  2653 11030  2180  7275  8250 10660 12787 13411  7483\n",
      "  1621  8022  3318 11895  7064  4149 11835 13219  2988   342   985  5531\n",
      "  5141  6153  7279  4333 13895 11070  6215  1521 11677  4194  3474  3647\n",
      "  6640 14793  5851 10115  4428  6088  4016  4365 11489 11908  5586  3712\n",
      "  1116  5129 10287   581 13843  3448 10101  5867 13939  1753  1533 12438\n",
      "  3269  5766   284 10802  3129  8432 10204  7308 12664  9009  8781 12412\n",
      "  6028  3593  1074  5718  6191  8229  1607 13931 11347  8852   646  5695\n",
      "  2489 11847  9056  7047  8345  7704  2935  3204 14370  6352 14876  6205\n",
      " 12245  1635 11377  1818  3653 11853  2048  6205 14840 13723 12075  7103\n",
      "  5386  4179  1899   895  7009  3179  4212  3439  9398   494  5604 14791\n",
      "  5122  5742   170 12178 10966 13427 14597  6760  5196 13929 10799  2879\n",
      " 12469  1355  4247 11323  1393  9346 14000  6096 12907  6340   982  3336\n",
      "  1462  4153 14410  3049  5415  1024  9056  1752  8719 12996  3939  7064\n",
      " 11403  2873 11559  5546  6392  9376  7596   256  5159  7066  7900  8612\n",
      "  8771  8075  3074  5166  9353 13788  4858 10005  2841  7547 10299  2784\n",
      " 14431 11796  7615  2974  5637  6334 14716  5271 11730 12498  4005  7973\n",
      "  5174 10874 11865 10180 10177 11745  8918 11872  8721  8599   802 12027\n",
      "  9476  1949  1372  8969  6814  8574 11955  9375 13581  6369  3869  9401\n",
      "  4396 12598  4099 10669 13901  6067  8566  8008  8266  5122 10305  3957\n",
      "  6177  7626 12660  9031 14072 14173]\n",
      "(558, 362)\n",
      "(1116, 361)\n",
      "(1116,)\n"
     ]
    }
   ],
   "source": [
    "train9 = resample_data(train_x1, 558)\n",
    "print(train9.shape)\n",
    "multi_x9, multi_y9 = concat_data(train9, train_x2)\n",
    "print(multi_x9.shape)\n",
    "print(multi_y9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.70      0.82      2867\n",
      "    class 1       0.11      0.81      0.19       133\n",
      "\n",
      "avg / total       0.95      0.70      0.79      3000\n",
      "\n",
      "0.7026666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm9 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x9, multi_y9)\n",
    "predictions9 = gbm9.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions9, target_names=target_names))\n",
    "\n",
    "val9_acc = metrics.accuracy_score(y_val,predictions9)#验证集上的auc值\n",
    "print(val9_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7122  2412 13080 13745  3206 10237 10726  6798   646 14424  7787  4138\n",
      "  6774 11772 14640 11606 10965  5203  8214 10940  9425   482 11184  8761\n",
      "  7176 11499  1262  8653  8130 14122  4149  2970  9237 10407 10793  1701\n",
      "  5017 14560  6609 10433  9093 10950  8002 13475  1091  7139  3916  2077\n",
      " 12811 11411  9200   223   617  3531  7740  1611 14712 14157 12209 12462\n",
      "  4889  7750   187  3370  6932  9522   806  2696  5315   101  1712  8596\n",
      "  6957 12483  4655 14802  2500  8479  2233  9319  1869 10982 12595   364\n",
      " 13586  8970  1689 14672  5764  5938  9129 14559 12763  5683 13727 11274\n",
      "  4860   918 14189 10713  4004 14605  2409 11036 14542  8251 12376 12714\n",
      " 14141  1766 14877  6358  2257  6798  3978 13701  7783  7345 13120 13189\n",
      "  2295  7108 13077 10486  9536 10556  9131 14738  4428 10700  7501  5929\n",
      " 13090 13751  8888  9254   283 13214  5662  2956  7961 12101  4096  2792\n",
      "  5241  9381   238  5171  4639  3895  8614 12572  9273  4237 12390  7257\n",
      "   329 10951  1253  6166  4184  3610  9416  1229 12623 14205   317 12990\n",
      "  5933  7125 13836 11381  4582  8797  2771  1362 14198  2598 12147 14422\n",
      " 14013  4192 13027  1083  6925 13411  1337  5704 12917 11020  9321  6865\n",
      " 10327 13812  7298  5951  8046  6358  6105  3868  1493  8060  5205 12505\n",
      "   745   712   836  4206  1809  7840 13672  8098  9512 11904  1706  1369\n",
      " 12694 14273 13094  2822  9324  1452  9080  2573  5806 12955  1146  8299\n",
      "  8477  6332  7923 10178  7290  6293  2938 14874 12362  3161  5618 14702\n",
      "  8562  6908  5683  8165   545  4272  8607 10442  5216 10621  8900  2213\n",
      "  7184  8440 12631  4585  5576  6694   170 12186   605  6265  8163  1862\n",
      " 11619  2428  7432  4611  1324  3443  8317  8697 10587  3745 12879    69\n",
      " 14268 11282  8032 11915  7719    67 11219  9307  2527  8662  5926  3170\n",
      " 14278 10839  3295  3061  9272  2378  7714  4582  2557  9362  5501 12155\n",
      " 14146  5551 14563 13223 13462 10788 12717  8229  6807  2929 12959 10973\n",
      "  5966  1028 10620  1894  9216  4904  5040  2329  2377  7090 10218  5137\n",
      "  7407  3536  7053 10575  8242   640 14697  7439  1769  4354  7637   629\n",
      " 11365 14593  3116  2108  6666 13218  7773  1643 12364  4132 11824  7044\n",
      "  7829  7352 14127  8006  1603  4691  3315  2475  3731  4481  3353  4607\n",
      " 10644  3172  7808  6463  5135 12100   897 12579  9092  2520  6325  4466\n",
      "   140  9506  1429  6631 14130 11835  5403 13230  5905  2031  3276 12951\n",
      " 10105  5814  4829  2682  8476  1512  1967 13047  4048 11971  4749 13939\n",
      "  9345  7900  3946  6821  8861  3655  4450  5922  7655  3992  5938  5286\n",
      "  4440  7326 13125  8844  1067  8213   627  6980  2825  1786  5074  5348\n",
      "  2064  8590 12422 12858 10764  4124  5521  7532 11908   347 11563  9471\n",
      "  1536 13432  8839 13270  1925  3511 14190  8123 10027  5241  8144 14012\n",
      "  7443    97   866  2841   632  1726  3209  7084 12736  8005  1646  2662\n",
      "  9378  3988 14824 14037  5920  5805   573  8635 10706  5093  8341 11340\n",
      "  2685 10159  5674 13829  1610  2902    71  6131  9277  4170 13062  1823\n",
      " 11365 12588  6888  6685 14303 14629 12846 13523 14222  2937  8658  5084\n",
      " 14227  6596  8401  9507  3708 12323  3216  8566  2278 12375  9314  8069\n",
      "  2822  8304  6380  5196  1503 11074  4284   687   785 14448  6398   273\n",
      "  3170 11944  7977  6080  2112  5259  6099  3779  8903 14083  6025 10993\n",
      "    85  9130 14000  6598  4173  6799 14192 10225  7629   570    21   157\n",
      "  7214  3135  4883 13814  6407 10559  1673  8615  6754  1920  1532   263\n",
      "  7168 13548  7966  5898  5849  6777]\n",
      "(558, 362)\n",
      "(1116, 361)\n",
      "(1116,)\n"
     ]
    }
   ],
   "source": [
    "train10 = resample_data(train_x1, 558)\n",
    "print(train10.shape)\n",
    "multi_x10, multi_y10 = concat_data(train10, train_x2)\n",
    "print(multi_x10.shape)\n",
    "print(multi_y10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.82      2867\n",
      "    class 1       0.10      0.74      0.18       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.7006666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm10 = XGBClassifier( n_estimators= 150, max_depth= 5, min_child_weight= 2, gamma=0.9, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1).fit(multi_x10, multi_y10)\n",
    "predictions10 = gbm10.predict(X_val)\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions10, target_names=target_names))\n",
    "\n",
    "val10_acc = metrics.accuracy_score(y_val,predictions10)#验证集上的auc值\n",
    "print(val10_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.70      0.82      2867\n",
      "    class 1       0.10      0.74      0.18       133\n",
      "\n",
      "avg / total       0.94      0.70      0.79      3000\n",
      "\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# predictions = predictions1+predictions2+predictions3+predictions4+predictions5+predictions6+predictions7+predictions8+predictions9+predictions10\n",
    "predictions = predictions1+predictions2+predictions3+predictions4+predictions5+predictions6+predictions7\n",
    "for i in range(3000):\n",
    "    if predictions[i] >= 4:\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_val, predictions, target_names=target_names))\n",
    "\n",
    "val_acc = metrics.accuracy_score(y_val,predictions)#验证集上的auc值\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "965"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
